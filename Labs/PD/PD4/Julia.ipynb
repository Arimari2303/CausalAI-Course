{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simulation exercise in regularization framework: sparsity in population coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using CSV\n",
    "using Distributions\n",
    "using DataFrames\n",
    "using Dates\n",
    "using Plots\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using LaTeXStrings\n",
    "using Lasso\n",
    "using GLM\n",
    "using MLBase\n",
    "using Statistics\n",
    "using GLMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_data(n::Int, p::Int, regime::String=\"sparse\")\n",
    "\n",
    "    # constants chosen to get R^2 of approximately .80\n",
    "    if regime == \"sparse\"\n",
    "        beta = ((1 ./ [1:p;]) .^ 2) * 7\n",
    "    elseif regime == \"dense\"\n",
    "        beta = sort(rand(Uniform(0.9, 1), p), rev=true)\n",
    "    end\n",
    "\n",
    "    function true_fn(x)\n",
    "        return x * beta\n",
    "    end\n",
    "\n",
    "    X = rand(Uniform(-0.5, 0.5), n, p)\n",
    "    gX = true_fn(X)\n",
    "    y = gX .+ randn(n)\n",
    "    Xpop = rand(Uniform(-0.5, 0.5), 100000, p)  # almost population limit\n",
    "    gXpop = true_fn(Xpop)\n",
    "    ypop = gXpop .+ randn(100000)\n",
    "    return X, y, gX, Xpop, ypop, gXpop, beta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate sparse coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to simulate a data generating process which coefficients vector is sparse. A set of coefficients is said to be approximate sparse if it satisfies the following property:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\beta_j \\propto \\frac{1}{j^2}, \\quad \\forall j=1,2,\\dots,p\n",
    "\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, the predictive power relies in a small subset of the coefficients vector (the larger ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 400\n",
    "X, y, gX, Xpop, ypop, gXpop, betas = gen_data(n, p, \"sparse\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the set of (sorted) coefficients' magnitudes to visualize their behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(betas),abs.(betas),seriestype=:path,color=:blue, labels = false,\n",
    "    marker=:circle,markersize=4,xlabel=L\"\\beta\",ylabel=\"Magnitude\",\n",
    "    title=L\"\\beta \\ Magnitude\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may already notice, we are woking in a high dimensional setting ($p>n$). Let's test the dimensional reduction capabilities of the regularization methods. We would like to select only features which provide the most predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the general regularization problem (in p-norm) is the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\min_{\\beta_j \\in \\beta} \\sum_{i=1}^{n}(Y_i - \\beta^{\\prime}X_i)^2 + \\lambda\\lVert \\beta \\lVert_p\n",
    "\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "\\lVert \\beta \\lVert_p = \\left(\\sum_{\\beta_j \\in \\beta}\\left|{\\beta_{j}}^{p}\\right|\\right) ^{1/p}\n",
    "\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $p=1$, it is a Lasso (or L1) regularization . When $p=2$, it is a Ridge (or L2) regularization. We will explore the latter it in the next sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we fit a Lasso regularization. To tune the penalization parameter $\\lambda$, we perform a cross-validation Lasso regression with $k=5$ folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = glmnetcv(X, y, alpha=1.0, intercept=false, nfolds=5)\n",
    "betas_l1 = model_l1.path.betas[:,argmin(model_l1.meanloss)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the estimated coefficients vs the population parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:19, abs.(betas[1:19]), label=\"Population\", color=:blue, marker=:circle, markersize=5)\n",
    "plot!(1:19, abs.(betas_l1[1:19]), label=\"Lasso\", color=:green, marker=:circle, markersize=5)\n",
    "title!(L\"First 20 larger $\\beta$s\")\n",
    "xlabel!(L\"$\\beta$\")\n",
    "ylabel!(\"Magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main conclusion of this exercise is that Lasso regularization fits scenearios of approximate sparse coefficients because it select a small subset with high predictive power. The linear predictor provided by Lasso may not be the BLP, but works well under high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Simple Case Study using Wage Data from 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.\n",
    "The preproccessed sample consists of $5150$ never-married individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/wage2015_subsample_inference.csv\"\n",
    "data = CSV.read(download(file), DataFrame,\n",
    "                types = Dict(:occ2 => String, :ind2 => String))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.lwage\n",
    "Z = select(data, Not([:wage, :lwage]))\n",
    "column_names = names(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the weekly wage distribution from the US survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(data.wage, bins=0:20:350, xlabel=\"hourly wage\", ylabel=\"Frequency\",\n",
    "          title=\"Empirical wage distribution from the US survey data\", label =false, ylim=(0, 3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wages show a high degree of skewness. Hence, wages are transformed in almost all studies by the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the skewness of the data, we are considering log wages which leads to the following regression model\n",
    "\n",
    "$$\\log(\\operatorname{wage}) = g(Z) + \\epsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will evaluate *linear* prediction rules. In later notebooks, we will also utilize nonlinear prediction methods. In linear models, we estimate the prediction rule of the form\n",
    "\n",
    "$$\\hat g(Z) = \\hat \\beta'X.$$\n",
    "\n",
    "Again, we generate $X$ in three ways:\n",
    "\n",
    "1. Basic Model:   $X$ consists of a set of raw regressors (e.g. gender, experience, education indicators, regional indicators).\n",
    "\n",
    "\n",
    "2. Flexible Model:  $X$ consists of all raw regressors from the basic model plus occupation and industry indicators, transformations (e.g., $\\operatorname{exp}^2$ and $\\operatorname{exp}^3$) and additional two-way interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the out-of-sample performance, we split the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic model consists of $51$ regressors, and the flexible model has $246$ regressors. Let us fit our models to the training sample using the two different model specifications. We are starting by running a simple ols regression and computing the $R^2$ on the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low dimensional specification (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_formula = @formula(lwage ~1+ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2)\n",
    "Z_base = modelmatrix(basic_formula, data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0 1.0 … 0.0 0.0; 1.0 0.0 … 0.0 1.0; … ; 1.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0], [1.0 1.0 … 0.0 0.0; 1.0 1.0 … 0.0 0.0; … ; 1.0 1.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sample = rand(Float64, size(data)[1]) .< 0.75\n",
    "test_sample = .!(train_sample)\n",
    "y_train, y_test = y[train_sample], y[test_sample]\n",
    "X_train, X_test = Z_base[train_sample, :], Z_base[test_sample, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = lm(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate R-squared on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_mse_testing = mean((predict(lr_base, X_test) - y_test) .^ 2)\n",
    "basic_r2_testing = 1 - basic_mse_testing / var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-dimensional specification (flexible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_formula = @formula(lwage ~1 + sex + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+occ2+ind2+mw+so+we))\n",
    "Z_flex = modelmatrix(flex_formula, data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = Z_flex[train_sample, :], Z_flex[test_sample, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_flex = lm(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2808300171229896"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "basic_mse_testing = mean((predict(lr_flex, X_test) - y_test) .^ 2)\n",
    "basic_r2_testing = 1 - basic_mse_testing / var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized regression: Lasso (flexible model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that ols regression works better for the basic model with smaller $p/n$ ratio. We proceed by running a Lasso regression for the flexible model, tuned via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly penalize the coefficients, we must standarize the data, so each regressor is symmetrically penalized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in R, Julia's GLMnet standarizes variables automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.507151527143755"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso_flex = glmnetcv(X_train, y_train, alpha=1.0, intercept=false, nfolds=5)\n",
    "betas_flex = lasso_flex.path.betas[:,argmin(lasso_flex.meanloss)];\n",
    "prediction = X_test * betas_flex\n",
    "basic_mse_testing = mean((prediction - y_test) .^ 2)\n",
    "basic_r2_testing = 1 - basic_mse_testing / var(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
