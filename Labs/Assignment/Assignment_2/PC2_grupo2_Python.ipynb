{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC2 - Grupo 2 (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrantes\n",
    "- GARCIA RODRIGUEZ, EMILIO ALONSO\n",
    "- PADILLA AQUISE, ALESSANDRO PIERO\n",
    "- RIEGA NUÑEZ, GABRIEL ANTONIO FERMIN\n",
    "- SALAMANCA FERNANDEZ, LUCAS PABLO\n",
    "- SILVA ANDUJAR, NICOLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading and processing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= \"https://raw.githubusercontent.com/d2cml-ai/CausalAI-Course/main/data/wage2015_subsample_inference.csv\"\n",
    "\n",
    "df =pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('rownames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As in Group Assignment 1 2024 - 2 #1044 , generate the extra-flexible model. This means that it contains all two-way interactions between the experience polynomials and the indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5150, 1431)\n"
     ]
    }
   ],
   "source": [
    "df_with_dummies = pd.get_dummies(df, columns=['occ2', 'ind2'], drop_first=True)\n",
    "extra_flexible_model_vars = [\"sex\",'exp1', 'exp2', 'exp3', 'exp4', 'hsg', 'scl', 'clg', 'ad', \n",
    "                             'so', 'we', 'ne'] + \\\n",
    "                             [col for col in df_with_dummies.columns if col.startswith('occ2_') or col.startswith('ind2_')]\n",
    "\n",
    "two_way_interactions = []\n",
    "for i, var1 in enumerate(extra_flexible_model_vars):\n",
    "    for var2 in extra_flexible_model_vars[i+1:]:\n",
    "        interaction_var = df_with_dummies[var1] * df_with_dummies[var2]\n",
    "        two_way_interactions.append(interaction_var.values.reshape(-1, 1))\n",
    "\n",
    "interactions_array = np.hstack(two_way_interactions)\n",
    "\n",
    "extra_flexible_model_array = np.hstack([df_with_dummies[extra_flexible_model_vars].values, interactions_array])\n",
    "print(extra_flexible_model_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Generate the array for the outcome variable Y and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logwage = df[['lwage']]\n",
    "log_w = np.array(df_logwage['lwage'])\n",
    "log_w = log_w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.24037498]\n",
      " [ 1.5815695 ]\n",
      " [-0.99532021]\n",
      " ...\n",
      " [ 1.19031569]\n",
      " [ 0.9200321 ]\n",
      " [-0.20976589]]\n"
     ]
    }
   ],
   "source": [
    "norm_log_w = (log_w - np.mean(log_w)) / np.std(log_w)\n",
    "\n",
    "print(norm_log_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5150, 1431)\n"
     ]
    }
   ],
   "source": [
    "experience_vars = ['exp1', 'exp2', 'exp3', 'exp4']\n",
    "\n",
    "experience_var_indices = [extra_flexible_model_vars.index(var) for var in experience_vars]\n",
    "\n",
    "extra_flexible_model_array_normalized = extra_flexible_model_array.copy()\n",
    "\n",
    "for idx in experience_var_indices:\n",
    "    col_mean = np.mean(extra_flexible_model_array[:, idx])\n",
    "    col_std = np.std(extra_flexible_model_array[:, idx])\n",
    "    extra_flexible_model_array_normalized[:, idx] = (extra_flexible_model_array[:, idx] - col_mean) / col_std\n",
    "\n",
    "print(extra_flexible_model_array_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 -0.6372836834860737 -0.6321497674155142 ... 0.0 0.0 0.0]\n",
      " [0.0 1.6250669865566842 1.647556165145875 ... 0.0 0.0 0.0]\n",
      " [0.0 0.39962704028352375 0.055261560933588944 ... 0.0 0.0 0.0]\n",
      " ...\n",
      " [0.0 -0.26022523847894735 -0.45217298326593086 ... 0.0 0.0 0.0]\n",
      " [0.0 -0.35448984973072895 -0.5046662119762261 ... 0.0 0.0 0.0]\n",
      " [0.0 0.02256859527639739 -0.2646971664434482 ... 0.0 0.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(extra_flexible_model_array_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split between training and testing samples. The testing sample should be 10% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ef_model =extra_flexible_model_array_normalized\n",
    "y=norm_log_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_train,ef_test,y_ef_train,y_ef_test = train_test_split(ef_model,y, train_size= 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Lasso Cross - Validation Procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Program a function that generates a logarithmically spaced grid. The input arguments should be the lower and upper bounds of the grid, as well as the natural logarithm of the spacing between each element of the grid. The output should be the logarithmically spaced grid, meaning that if we take the natural logarithm of each entry in the grid, they will be equally spaced. This will be the grid of values for λ values to try during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_spaced_grid(lower_bound, upper_bound, log_spacing, num_points):\n",
    "    log_lower = np.log(lower_bound)\n",
    "    log_upper = np.log(upper_bound)\n",
    "\n",
    " \n",
    "    log_grid = np.linspace(log_lower, log_upper, num_points)\n",
    "    \n",
    "   \n",
    "    return np.exp(log_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Program a function to generate \n",
    "k\n",
    " folds. It should take as input the array to be split rowwise and the number of folds desired. It should output a list of \n",
    "k\n",
    " 1d arrays of booleans; these arrays should all be the same length as the number of rows in the input array, and when they are all summed together they should add up to an array of all true values. Create your own procedure for splitting. You can aid yourself with third party packages like numpy in Python or Statistics in Julia, but do not use a pre-programmed third party splitting procedure like sk-learns's KFolds in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_folds(X, k):\n",
    "    n_samples = X.shape[0]\n",
    "    fold_sizes = np.full(k, n_samples // k, dtype=int)\n",
    "    fold_sizes[:n_samples % k] += 1  # Distribute remaining samples\n",
    "    \n",
    "    # Create an array of indices and shuffle it\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Create the folds\n",
    "    folds = []\n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[indices[start:stop]] = True\n",
    "        folds.append(mask)\n",
    "        current = stop\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Program a function that integrates those that you programmed in the last two items to find the value of \n",
    "λ\n",
    " that minimizes the testing mean square error across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def find_optimal_lambda(Y, X, lambda_bounds, k):\n",
    "    lambdas = log_spaced_grid(lambda_bounds[0], lambda_bounds[1])\n",
    "    \n",
    "    folds = generate_k_folds(X, k)\n",
    "    \n",
    "    all_mse = np.zeros((len(lambdas), k))\n",
    "    \n",
    "    for i, lambda_val in enumerate(lambdas):\n",
    "        for j, fold in enumerate(folds):\n",
    "            X_train, X_test = X[~fold], X[fold]\n",
    "            Y_train, Y_test = Y[~fold], Y[fold]\n",
    "            \n",
    "            model = Lasso(alpha=lambda_val, fit_intercept=True)\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "            all_mse[i, j] = np.mean((Y_test - Y_pred)**2)\n",
    "\n",
    "    avg_mse = np.mean(all_mse, axis=1)\n",
    "    \n",
    "    \n",
    "    optimal_index = np.argmin(avg_mse)\n",
    "    optimal_lambda = lambdas[optimal_index]\n",
    "    \n",
    "    optimal_model = Lasso(alpha=optimal_lambda, fit_intercept=True)\n",
    "    optimal_model.fit(X, Y)\n",
    "    \n",
    "    \n",
    "    result = {\n",
    "        'optimal_lambda': optimal_lambda,\n",
    "        'optimal_coef': optimal_model.coef_,\n",
    "        'all_lambdas': lambdas,\n",
    "        'all_mse': avg_mse\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Program a function for predicting the outcome variable through model estimated with the optimal lambda. It should take as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lasso_predict(optimal_model, X):\n",
    "    optimal_lambda = optimal_model['optimal_lambda']\n",
    "    optimal_coef = optimal_model['optimal_coef']\n",
    "\n",
    "    model = Lasso(alpha=optimal_lambda, fit_intercept=True)\n",
    "    model.coef_ = optimal_coef\n",
    "    model.intercept_ = 0  \n",
    "\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
