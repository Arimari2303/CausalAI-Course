{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 2 - PC3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrantes\n",
    "- GARCIA RODRIGUEZ, EMILIO ALONSO (Python)\n",
    "- PADILLA AQUISE, ALESSANDRO PIERO (R)\n",
    "- RIEGA NUÃ‘EZ, GABRIEL ANTONIO FERMIN (R)\n",
    "- SALAMANCA FERNANDEZ, LUCAS PABLO (Julia)\n",
    "- SILVA ANDUJAR, NICOLAS (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using CategoricalArrays\n",
    "using Pkg\n",
    "using Distributions\n",
    "using Dates\n",
    "using Plots\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using LaTeXStrings\n",
    "using Lasso\n",
    "using Statistics\n",
    "using GLMNet\n",
    "using StatsModels\n",
    "using HDMjl\n",
    "\n",
    "data = CSV.read(\"wage2015_subsample_inference.csv\", DataFrame,\n",
    "                types = Dict(:occ2 => String, :ind2 => String));\n",
    "\n",
    "data = select(data, Not([\"wage\", \"rownames\"])); \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_vars = [\"sex\", \"clg\", \"mw\", \"so\", \"we\", \"ne\", \"occ2\", \"ind2\"]\n",
    "\n",
    "for var in categorical_vars\n",
    "    data[!, var] = categorical(data[!, var])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the design matrix with adjusted formula\n",
    "design = @formula(\n",
    "    lwage ~ 1 + sex + clg * sex + clg * (mw + so + we + ne) + sex * (mw + so + we + ne)+ clg * sex * (mw + so + we + ne) + (exp1 + exp2 + exp3 + exp4) * (hsg + scl + clg + ad + mw + so + we + ne + occ2 + ind2)\n",
    "    + (hsg + scl + clg + ad) * (mw + so + we + ne + occ2 + ind2) + (mw + so + we + ne) * (occ2 + ind2) + occ2 * ind2\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ModelFrame which retains the formula and data structure\n",
    "mf = ModelFrame(design, data)\n",
    "\n",
    "# Extract column names using coefnames from the ModelFrame\n",
    "X_names = coefnames(mf)\n",
    "X_names = X_names[2:end]  # Exclude the intercept\n",
    "\n",
    "# Verify the content of X_names\n",
    "println(\"Coefficient Names:\")\n",
    "println(X_names)\n",
    "\n",
    "# Create the ModelMatrix from the ModelFrame\n",
    "mm = modelmatrix(mf)\n",
    "\n",
    "# Convert the predictors to a plain Matrix, excluding the intercept\n",
    "X = mm[:, 2:end]\n",
    "\n",
    "# Extract the target variable\n",
    "y = data.lwage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Identify Treatment and Control Variables \n",
    "\n",
    "# Define regular expressions to match treatment variables and their interactions\n",
    "treatment_patterns = [\n",
    "    r\"^clg: 1\\.0$\",                      # clg\n",
    "    r\"^clg: 1\\.0 & sex: 1\\.0$\",          # clg:sex\n",
    "    r\"^clg: 1\\.0 & (mw|so|we|ne): 1\\.0$\",# clg:mw, clg:so, clg:we, clg:ne\n",
    "    r\"^clg: 1\\.0 & sex: 1\\.0 & (mw|so|we|ne): 1\\.0$\" # clg:sex:mw, clg:sex:so, etc.\n",
    "]\n",
    "\n",
    "\n",
    "# Identify indices of treatment variables based on the patterns\n",
    "treatment_indices = [\n",
    "    i for (i, name) in enumerate(X_names)\n",
    "    if any(occursin(pat, name) for pat in treatment_patterns)\n",
    "]\n",
    "\n",
    "# Identify control indices as all other variables\n",
    "control_indices = setdiff(1:length(X_names), treatment_indices)\n",
    "\n",
    "# Verify identified treatment variables\n",
    "println(\"\\nTreatment Variables Identified:\")\n",
    "println(X_names[treatment_indices])\n",
    "println(X_names[control_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate treatment and control matrices\n",
    "D = X[:, treatment_indices]      # Treatment variables\n",
    "Z = X[:, control_indices]        # Control variables\n",
    "\n",
    "# Check if D and Z have the expected dimensions\n",
    "println(\"\\nDimensions of D (Treatment): \", size(D))\n",
    "println(\"Dimensions of Z (Control): \", size(Z))\n",
    "println(\"Length of y (Target): \", length(y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Double Lasso Estimation for Each Treatment Variable\n",
    "# Initialize a DataFrame to store all results with StdError and TStatistic\n",
    "results_df = DataFrame(\n",
    "    Variable = String[],\n",
    "    Coefficient = Float64[],\n",
    "    StdError = Float64[],\n",
    "    TStatistic = Float64[],\n",
    "    PValue = Float64[]\n",
    ")\n",
    "\n",
    "# Iterate over each treatment variable and perform double lasso\n",
    "for i in 1:size(D, 2)\n",
    "    treatment_var = X_names[treatment_indices[i]]\n",
    "    d_i = D[:, i]  # Extract the i-th treatment variable as a Vector\n",
    "    \n",
    "    println(\"\\nPerforming Double Lasso for Treatment Variable: $treatment_var\")\n",
    "    \n",
    "    # Perform double lasso estimation using HDMjl\n",
    "    result_i = try\n",
    "        rlassoEffect(Z, y, d_i; method = \"double selection\")\n",
    "    catch e\n",
    "        println(\"Error during rlassoEffect for $treatment_var: \", e)\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    # Print the keys to verify\n",
    "    println(\"Keys in the result: \", keys(result_i))\n",
    "    \n",
    "    # Check if necessary keys are present\n",
    "    if !(\"coefficients\" in keys(result_i)) || !(\"se\" in keys(result_i)) || !(\"t\" in keys(result_i))\n",
    "        println(\"Missing necessary keys in the result for $treatment_var. Skipping...\")\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    # Extract coefficients, standard errors, and t-statistics\n",
    "    coefficients = result_i[\"coefficients\"]\n",
    "    se = result_i[\"se\"]\n",
    "    t_stats = result_i[\"t\"]\n",
    "    \n",
    "    # Compute p-values using the t-statistics\n",
    "    p_values = 2 .* (1 .- cdf.(Normal(), abs.(t_stats)))\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    push!(results_df, (\n",
    "        Variable = treatment_var,\n",
    "        Coefficient = coefficients,\n",
    "        StdError = se,\n",
    "        TStatistic = t_stats,\n",
    "        PValue = p_values\n",
    "    ))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize and Interpret Results\n",
    "\n",
    "# Display the summary of estimated parameters\n",
    "println(\"\\nSummary of Estimated Parameters:\")\n",
    "println(results_df)\n",
    "\n",
    "# Identify significant variables (e.g., p-value < 0.05)\n",
    "significant_results = filter(row -> row.PValue < 0.05, results_df)\n",
    "\n",
    "println(\"\\nSignificant Parameters (p-value < 0.05):\")\n",
    "println(significant_results)\n",
    "\n",
    "# Determine which treatment variables have significant impacts\n",
    "if nrow(significant_results) > 0\n",
    "    println(\"\\nInterpretation:\")\n",
    "    for row in eachrow(significant_results)\n",
    "        println(\"Variable: \", row.Variable)\n",
    "        println(\"  Coefficient: \", row.Coefficient)\n",
    "        println(\"  StdError: \", row.StdError)\n",
    "        println(\"  T-Statistic: \", row.TStatistic)\n",
    "        println(\"  P-Value: \", row.PValue)\n",
    "    end\n",
    "else\n",
    "    println(\"No significant parameters found at the 5% significance level.\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "First, the coefficient of having a college degree by itself is asociated with an increase in wages by approximately 49%. Next, when interacting this term with sex, the coefficient is 0.088 and significant, hihglighting that a wage gender premium exists even when accounting for college graduates. The same applies when the college graduate variable is interacted with both sex and south and sex and west, showing that this wage premium is even stronger in these regions, which makes sense, as these are more conservative regions. There are also some interesting wage premiums by just geographical location, in the case of the mid-west, south and north east, due to them being regions with considerably stronger and larger industries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RDatasets, DataFrames, MLJ, MLJDecisionTreeInterface, DecisionTree, StatsPlots, MLJModels\n",
    "\n",
    "hitters = dataset(\"ISLR\", \"Hitters\")\n",
    "hitters[1:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(hitters, :nmissing)\n",
    "\n",
    "#59 salaries missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = dropmissing(hitters, :Salary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the target variable `Salary` and the features\n",
    "y, X = unpack(hitters, ==(:Salary), !=(:Salary))\n",
    "\n",
    "# Transform categorical variables to dummies using one-hot encoding\n",
    "onehotencoder = @load OneHotEncoder pkg=MLJModels verbosity=0\n",
    "ohe = onehotencoder(features=[:League, :Division, :NewLeague])\n",
    "ohe_machine = machine(ohe, X)\n",
    "MLJ.fit!(ohe_machine);\n",
    "X = MLJ.transform(ohe_machine, X);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.schema(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce counts to continuous variables\n",
    "MLJ.coerce!(X, Count => MLJ.Continuous)\n",
    "\n",
    "# Combine `X` and `y` into a single DataFrame\n",
    "df = hcat(X, y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = partition(eachindex(y), 0.9, rng=1)\n",
    "train_df = df[train_indices, :];\n",
    "test_df = df[test_indices, :];\n",
    "\n",
    "rename!(train_df, :x1 => :Salary);\n",
    "rename!(test_df, :x1 => :Salary);\n",
    "\n",
    "# Transform Salary to LogSalary and then drop Salary\n",
    "train_df.LogSalary = log.(train_df.Salary)\n",
    "test_df.LogSalary = log.(test_df.Salary)\n",
    "\n",
    "select!(train_df, Not(:Salary));\n",
    "select!(test_df, Not(:Salary));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(names(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model with all regressors from training set\n",
    "fmla = @formula(LogSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League__A + League__N + Division__E + Division__W + PutOuts + Assists + Errors + NewLeague__A + NewLeague__N)\n",
    "ols_model= lm(fmla, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the OLS point estimate using the training set\n",
    "b_hat = GLM.coef(ols_model)\n",
    "\n",
    "# Number of features (including the intercept)\n",
    "n_features = length(b_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping\n",
    "# loop to generate 10,000 bootstrap estimates\n",
    "n_boots = 10_000\n",
    "b_boots = zeros(n_boots, n_features)\n",
    "n_train = nrow(train_df)\n",
    "Random.seed!(123)  # For reproducibility\n",
    "\n",
    "for i in 1:n_boots\n",
    "    # Sample indices with replacement\n",
    "    indices = rand(1:n_train, n_train)\n",
    "    boot_df = train_df[indices, :]\n",
    "    # Fit OLS model on the bootstrap sample\n",
    "    boot_model = lm(fmla, boot_df)\n",
    "    # Store the bootstrap coefficients\n",
    "    b_boots[i, :] = GLM.coef(boot_model)\n",
    "end\n",
    "\n",
    "n_b_boots = size(b_boots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store the lower and upper bounds\n",
    "b_lower = zeros(n_features)\n",
    "b_upper = zeros(n_features)\n",
    "\n",
    "# Calculate the 2.5% and 97.5% percentiles for each coefficient\n",
    "for j in 1:n_features\n",
    "    b_lower[j] = quantile(b_boots[:, j], 0.025)\n",
    "    b_upper[j] = quantile(b_boots[:, j], 0.975)\n",
    "end\n",
    "length(b_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals\n",
    "b_conf_lower = b_hat .- b_upper\n",
    "b_conf_upper = b_hat .- b_lower\n",
    "\n",
    "# Display the results\n",
    "coef_names = coefnames(ols_model)\n",
    "for i in 1:n_features\n",
    "    println(\"Coefficient: $(coef_names[i])\")\n",
    "    println(\"Point Estimate: $(b_hat[i])\")\n",
    "    println(\"95% Confidence Interval: [$(b_conf_lower[i]), $(b_conf_upper[i])]\")\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = MLJModels.predict(ols_model, test_df)\n",
    "\n",
    "# Calculate the MSE\n",
    "mse = mean((test_df.LogSalary - y_pred).^2)\n",
    "\n",
    "println(\"Out-of-sample MSE: $mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DecisionTreeRegressor\n",
    "train, test = partition(eachindex(y), 0.9, rng = 1);\n",
    "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree verbosity=0\n",
    "\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_machine = machine(tree_model, X[train, :], y[train])\n",
    "MLJ.fit!(tree_machine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tree\n",
    "fitted_params(tree_machine)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "predictions = MLJ.predict(tree_machine, X[test, :])\n",
    "sqrt(mean((predictions - y[test]) .^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate different trees and then obtain threshold with lowerst RMSE\n",
    "tree_model_prune = DecisionTreeRegressor(post_prune = true, merge_purity_threshold = 0.6)\n",
    "tree_machine_prune = machine(tree_model_prune, X[train, :], y[train])\n",
    "MLJ.fit!(tree_machine_prune);\n",
    "\n",
    "thresholds = exp.(collect(-10:0.01:0))\n",
    "rmses = []\n",
    "\n",
    "for threshold in thresholds\n",
    "        tree_model_prune.merge_purity_threshold = threshold\n",
    "        evaluation = evaluate!(\n",
    "                tree_machine_prune,\n",
    "                resampling = CV(nfolds = 3, shuffle = true, rng = 123),\n",
    "                measure = rmse\n",
    "        )\n",
    "        rmses = [rmses; evaluation.measurement]\n",
    "end\n",
    "\n",
    "thresholds[argmin(rmses)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use threshold that minimizes RMSE to create pruned tree\n",
    "tree_model_prune = DecisionTreeRegressor(post_prune = true, merge_purity_threshold = thresholds[argmin(rmses)])\n",
    "tree_machine_prune = machine(tree_model_prune, X[train, :], y[train])\n",
    "MLJ.fit!(tree_machine_prune);\n",
    "predictions_prune = MLJ.predict(tree_machine_prune, X[test, :])\n",
    "sqrt(mean((predictions_prune - y[test]) .^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_df = DataFrame(\n",
    "    Model = String[], \n",
    "    RMSE = Float64[]\n",
    ")\n",
    "\n",
    "# 1. Calculate RMSE for OLS with bootstrapping\n",
    "ols_rmse = sqrt(mean((test_df.LogSalary - y_pred).^2))  \n",
    "push!(results_df, (\"OLS with Bootstrapping\", ols_rmse))\n",
    "\n",
    "# 2. Calculate RMSE for unpruned tree\n",
    "unpruned_tree_rmse = sqrt(mean((y[test] - predictions).^2))  \n",
    "push!(results_df, (\"Unpruned Tree\", unpruned_tree_rmse))\n",
    "\n",
    "# 3. Calculate RMSE for pruned tree\n",
    "pruned_tree_rmse = sqrt(mean((y[test] - predictions_prune).^2)) \n",
    "push!(results_df, (\"Pruned Tree\", pruned_tree_rmse))\n",
    "\n",
    "# summary of RMSE\n",
    "println(\"Model Comparison Summary:\")\n",
    "println(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
