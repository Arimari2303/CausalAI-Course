{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e0701",
   "metadata": {},
   "source": [
    "## Proof of the Frisch - Waugh - Lovell Theorem\n",
    "\n",
    "For the proof of the theorem, we use the following:\n",
    "\n",
    "1. The Partialling-out operation\n",
    "\n",
    "    Be the following equation:\n",
    "    $V$ = $\\beta$$W$ + e\n",
    "        \n",
    "    $\\tilde{V}$ =  $V$ - $\\alpha_{YW}$$W$\n",
    "    \n",
    "    $\\alpha_{YW}$ is the $\\hat{\\beta}$ (estimated parameter of the regression of V with W)\n",
    "    \n",
    "    We're creating a \"residual\" V by subtracting the part of V\n",
    "    that is linearly predicted\n",
    "    \n",
    "\n",
    "2. $Y$ = $V$ + $W$  $\\longrightarrow$  $\\tilde{Y}$ = $\\tilde{V}$ + $\\tilde{W}$ \n",
    "    \n",
    "    This property tells us that from a linear combination of the sum of two vectors it follows that there is a linear       combination of the residualized vectors.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c430dd0",
   "metadata": {},
   "source": [
    "Then let be the following regression:\n",
    "\n",
    "\\begin{align} \n",
    "Y= T \\beta_1 + X \\beta_2 + e   \n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "$T$: treatment variable\n",
    "\n",
    "$\\beta_1$: parameter that captures the causal effect\n",
    "\n",
    "$X$: other regressors\n",
    "\n",
    "$e$: error\n",
    "\n",
    "Since we are interested in knowing only the value of $\\beta_1$, we partialling-out to both sides of our regression equation:\n",
    "\n",
    "\\begin{align} \n",
    "\\tilde{Y}= \\tilde{T}\\beta_1 +  \\tilde{X}\\beta_2 + \\tilde{e}   \n",
    "\\end{align}\n",
    "\n",
    "- Since the regression is a linear combination, we can obtain a linear equation of the errors estimated by the property defined at the beginning.\n",
    "\n",
    "- Each argument of the [2] equation is the estimated error of the regression of that variable with respect to $X$ \n",
    "\n",
    " $\\tilde{Y}$ =  $Y$ - $\\alpha_{YX}$$X$\n",
    " \n",
    " $\\tilde{T}$ =  $T$ - $\\alpha_{YX}$$X$\n",
    "\n",
    " $\\tilde{X}$ =  $X$ - $\\alpha_{XX}$$X$\n",
    "\n",
    " $\\tilde{e}$ =  $e$ - $\\alpha_{eX}$$X$\n",
    " \n",
    "\n",
    "- Some of these estimated error will be deleted because:\n",
    "\n",
    " $\\alpha_{XX}$ = $I$. So, $\\tilde{X}$ = 0\n",
    " \n",
    " By definition: \n",
    " $E(e | X)$ = 0, $\\alpha_{eX}$= 0, and $\\tilde{e}$=$e$\n",
    " \n",
    "\n",
    "- So the [2] equation reduces to [3]:\n",
    "\n",
    "\\begin{align} \n",
    "\\tilde{Y}= \\tilde{T}\\beta_1 + {e}   \n",
    "\\end{align}\n",
    "\n",
    "Finally we come to what F-W-L proves in their theorem: we can reduce the [1] equation containing many regressors on $X$ to a simple residual regression (which is defined by partially removing the linear effect of $X$ from $Y$ and $T$) that only has the parameter that we are interested in estimating.\n",
    "\n",
    "The estimated parameter $\\beta_1$ of the [1] equation will be equal to the parameter $\\beta_1$ that will be estimated in the [3] regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
