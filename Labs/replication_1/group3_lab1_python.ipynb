{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e0701",
   "metadata": {},
   "source": [
    "## Group3 - Replication_1\n",
    "#### Members\n",
    "1. Andrea Ulloa (20172597)\n",
    "2. Ana Angulo (20171627)\n",
    "3. Angela Coapaza (20171636) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62f5da",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d338a7",
   "metadata": {},
   "source": [
    "### An Inferencial Problem: The College-Educated Wage Gap\n",
    "Using the data from the March Supplement of the U.S. Current Population Survey (2015) , in  this lab, we are going to focus in  payments of the college-educated workers and answer the following inference question:\n",
    "\n",
    "What is the difference in predicted wages between workers with some college education (scl) versus college graduate workers(clg)?\n",
    "\n",
    "To investigate the College-Educated Wage Gap, we consider the following log-linear regression model:\n",
    "\n",
    "\\begin{align}\n",
    "\\log(Y) &= \\beta'X + \\epsilon\\\\\n",
    "&= \\beta_1 SCL  + \\beta_2 CLG + \\beta_3'W  + \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "Where  SCL is the indicator of workers with some college education( 1  if yes 0  otherwise), CLG is the indicator of college graduate workers (1  if yes 0  otherwise) and the  ùëä 's are controls explaining variation in wages. Considering transformed wages by the logarithm, we are analyzing the relative difference in the payment of workers with some college education and college graduate workers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e2015",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df568e9",
   "metadata": {},
   "source": [
    "We consider the same subsample of the U.S. Current Population Survey (2015). Let us load the data set.\n",
    "\n",
    "***Variable description***\n",
    "\n",
    "- occ : occupational classification\n",
    "- ind : industry classification\n",
    "- lwage : log hourly wage\n",
    "- sex : gender (1 female) (0 male)\n",
    "- shs : some high school\n",
    "- hsg : High school graduated\n",
    "- scl : Some College\n",
    "- clg: College Graduate\n",
    "- ad: Advanced Degree\n",
    "- ne: Northeast\n",
    "- mw: Midwest\n",
    "- so: South\n",
    "- we: West\n",
    "- exp1: experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c430dd0",
   "metadata": {},
   "source": [
    "## Proof of the Frisch - Waugh - Lovell Theorem\n",
    "\n",
    "For the proof of the theorem, we use the following:\n",
    "\n",
    "1. The Partialling-out operation\n",
    "\n",
    "    Be the following equation:\n",
    "    $V$ = $\\beta$$W$ + e\n",
    "        \n",
    "    $\\tilde{V}$ =  $V$ - $\\alpha_{YW}$$W$\n",
    "    \n",
    "    $\\alpha_{YW}$ is the $\\hat{\\beta}$ (estimated parameter of the regression of V with W)\n",
    "    \n",
    "    We're creating a \"residual\" V by subtracting the part of V\n",
    "    that is linearly predicted\n",
    "    \n",
    "\n",
    "2. This property tells us that from a linear combination of the sum of two vectors it follows that there is a linear       combination of the residualized vectors.\n",
    "\n",
    " $Y$ = $V$ + $W$  $\\longrightarrow$  $\\tilde{Y}$ = $\\tilde{V}$ + $\\tilde{W}$ \n",
    "    \n",
    "    \n",
    "\n",
    "Then let be the following regression:\n",
    "\n",
    "\\begin{align} \n",
    "Y= T \\beta_1 + X \\beta_2 + e   \n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "$T$: treatment variable\n",
    "\n",
    "$\\beta_1$: parameter that captures the causal effect\n",
    "\n",
    "$X$: other regressors\n",
    "\n",
    "$e$: error\n",
    "\n",
    "Since we are interested in knowing only the value of $\\beta_1$, we partialling-out to both sides of our regression equation:\n",
    "\n",
    "\\begin{align} \n",
    "\\tilde{Y}= \\tilde{T}\\beta_1 +  \\tilde{X}\\beta_2 + \\tilde{e}   \n",
    "\\end{align}\n",
    "\n",
    "- Since the regression is a linear combination, we can obtain a linear equation of the errors estimated by the property defined at the beginning.\n",
    "\n",
    "- Each argument of the equation [2] is the estimated error of the regression of that variable with respect to $X$ \n",
    "\n",
    " $\\tilde{Y}$ =  $Y$ - $\\alpha_{YX}$$X$\n",
    " \n",
    " $\\tilde{T}$ =  $T$ - $\\alpha_{YX}$$X$\n",
    "\n",
    " $\\tilde{X}$ =  $X$ - $\\alpha_{XX}$$X$\n",
    "\n",
    " $\\tilde{e}$ =  $e$ - $\\alpha_{eX}$$X$\n",
    " \n",
    "\n",
    "- Some of these estimated error will be deleted because:\n",
    "\n",
    " $\\alpha_{XX}$ = $I$. So, $\\tilde{X}$ = 0\n",
    " \n",
    " By definition: \n",
    " $E(e | X)$ = 0, $\\alpha_{eX}$= 0, and $\\tilde{e}$=$e$\n",
    " \n",
    "\n",
    "- So the  equation [2] reduces to the equation [3]:\n",
    "\n",
    "\\begin{align} \n",
    "\\tilde{Y}= \\tilde{T}\\beta_1 + {e}   \n",
    "\\end{align}\n",
    "\n",
    "Finally we come to what F-W-L proves in their theorem: we can reduce the  equation [1] containing many regressors on $X$ to a simple residual regression (which is defined by partially removing the linear effect of $X$ from $Y$ and $T$) that only has the parameter that we are interested in estimating.\n",
    "\n",
    "The estimated parameter $\\beta_1$ of the  equation [1] will be equal to the parameter $\\beta_1$ that will be estimated in the regression [3]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
