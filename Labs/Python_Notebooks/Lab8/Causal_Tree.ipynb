{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79789d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U DoubleML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42dcfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "from SyncRNG import SyncRNG\n",
    "import numpy as np\n",
    "import re\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from scipy import linalg\n",
    "from itertools import chain\n",
    "\n",
    "from SyncRNG import SyncRNG\n",
    "\n",
    "from CTL.causal_tree_learn import CausalTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotnine as p\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f01b36",
   "metadata": {},
   "source": [
    "# HTE I: Binary treatment\n",
    "\n",
    "Source RMD file: [link](https://docs.google.com/uc?export=download&id=1FSUi4WLfYYKnvWsNWypiQORhkqf5IlFP)\n",
    "\n",
    "In the previous chapter, we learned how to estimate the effect of a binary treatment averaged over the entire population. However, the average may obscure important details about how different individuals react to the treatment. In this chapter, we will learn how to estimate the **conditional average treatment effect (CATE)**,\n",
    "\\begin{equation}\n",
    "  (\\#eq:cate)\n",
    "  \\tau(x) := \\E[Y_i(1) - Y_i(0) | X_i = x],\n",
    "\\end{equation}\n",
    "which is a \"localized\" version of the average treatment effect conditional on a vector of observable characteristics. \n",
    "\n",
    "It's often the case that \\@ref(eq:cate) is too general to be immediately useful, especially when the observable covariates are high-dimensional. It can be hard to estimate reliably without making strong modeling assumptions, and hard to summarize in a useful manner after estimation. In such situations, we will instead try to estimate treatment effect averages for simpler groups\n",
    "\\begin{equation}\n",
    "  (\\#eq:cate-g)\n",
    "  \\E[Y_i(1) - Y_i(0) | G_i = g],\n",
    "\\end{equation}\n",
    "where $G_i$ indexes subgroups of interest. Below you'll learn how to estimate and test hypotheses about pre-defined subgroups, and also how to discover subgroups of interest from the data. In this tutorial, you will learn how to use estimates of \\@ref(eq:cate) to suggest relevant subgroups $G_i$ (and in the next chapters you will find out other uses of \\@ref(eq:cate) in policy learning and evaluation).\n",
    "\n",
    "We'll continue using the abridged version of the General Social Survey (GSS) [(Smith, 2016)](https://gss.norc.org/Documents/reports/project-reports/GSSProject%20report32.pdf) dataset that was introduced in the previous chapter. In this dataset, individuals were sent to treatment or control with equal probability, so we are in a randomized setting. However, many of the techniques and code shown below should also work in an observational setting provided that unconfoundedness and overlap are satisfied (these assumptions were defined in the previous chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a981637",
   "metadata": {},
   "source": [
    "As with other chapters in this tutorial, the code below should still work by replacing the next snippet of code with a different dataset, provided that you update the key variables `treatment`, `outcome`, and `covariates` below. Also, please make sure to read the comments as they may be subtle differences depending on whether your dataset was created in a randomized or observational setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48366a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( \"https://docs.google.com/uc?id=1kSxrVci_EUcSr_Lg1JKk1l7Xd5I9zfRC&export=download\" )\n",
    "\n",
    "n = data.shape[0]\n",
    "\n",
    "# Treatment: does the the gov't spend too much on \"welfare\" (1) or \"assistance to the poor\" (0)\n",
    "treatment = \"w\"\n",
    "\n",
    "# Outcome: 1 for 'yes', 0 for 'no'\n",
    "outcome = \"y\"\n",
    "\n",
    "# Additional covariates\n",
    "covariates = [\"age\", \"polviews\", \"income\", \"educ\", \"marital\", \"sex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfb00e",
   "metadata": {},
   "source": [
    "## Pre-specified hypotheses\n",
    "\n",
    "We will begin by learning how to test pre-specified null hypotheses of the form\n",
    "\\begin{equation}\n",
    "  (\\#eq:hte-hyp)\n",
    "  H_{0}: \\E[Y(1) - Y(0) | G_i = 1] = \\E[Y(1) - Y(0) | G_i = 0].\n",
    "\\end{equation}\n",
    "\n",
    "That is, that the treatment effect is the same regardless of membership to some group\n",
    "$G_i$. Importantly, for now we’ll assume that the group $G_i$ was **pre-specified** -- it was decided _before_ looking at the data.\n",
    "\n",
    "In a randomized setting, if the both the treatment  $W_i$ and group membership $G_i$ are binary, we can write\n",
    "\\begin{equation}\n",
    "  (\\#eq:linear)\n",
    "  \\E[Y_i(W_i)|G_i] = \\E[Y_i|W_i, G_i] = \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i\n",
    "\\end{equation}\n",
    "\n",
    "<font size=1>\n",
    "When $W_i$ and $G_i$ are binary, this decomposition is true without loss of generality. Why?\n",
    "</font>\n",
    "\n",
    "This allows us to write the average effects of $W_i$ and $G_i$ on $Y_i$ as\n",
    "\\begin{equation}\n",
    "  (\\#eq:decomp)\n",
    "  \\begin{aligned}\n",
    "    \\E[Y(1) | G_i=1] &= \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i, \\\\\n",
    "    \\E[Y(1) | G_i=0] &= \\beta_0 + \\beta_w W_i,  \\\\\n",
    "    \\E[Y(0) | G_i=1] &= \\beta_0 + \\beta_g G_i,  \\\\\n",
    "    \\E[Y(0) | G_i=0] &= \\beta_0.\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting the null hypothesis \\@ref(eq:hte-hyp) in terms of the decomposition \\@ref(eq:decomp), we see that it boils down to a test about the coefficient in the interaction: $\\beta_{xw} = 0$. Here’s an example that tests whether the treatment effect is the same for \"conservative\" (`polviews` < 4) and \"liberal\" (`polviews` $\\geq$ 4) individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2fe372",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"conservative\"] = np.multiply(data.polviews < 4, 1)  # a binary group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8ae044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Test for Constraints                               \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept          0.4836      0.005     95.127      0.000       0.474       0.494\n",
      "w                 -0.3789      0.006    -64.657      0.000      -0.390      -0.367\n",
      "conservative      -0.1590      0.009    -17.195      0.000      -0.177      -0.141\n",
      "w:conservative     0.1160      0.010     11.185      0.000       0.096       0.136\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Only valid in randomized settings\n",
    "\n",
    "# Suppose this his group was defined prior to collecting the data\n",
    "data[\"conservative\"] = np.multiply(data.polviews < 4, 1)  # a binary group\n",
    "group = 'conservative'\n",
    "\n",
    "# Recall from last chapter -- this is equivalent to running a t-test\n",
    "fmla = 'y ~ w*conservative'\n",
    "ols = smf.ols(fmla, data=data).fit(cov_type='HC2')\n",
    "# print(ols_1.summary())\n",
    "hypotheses = 'Intercept=0, w=0, conservative=0, w:conservative=0'\n",
    "t_test = ols.t_test(hypotheses)\n",
    "print(t_test.summary(xname=list(ols.summary2().tables[1].index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f14d9",
   "metadata": {},
   "source": [
    "<font size=1>\n",
    "Interpret the results above. The coefficient $\\beta_{xw}$ is denoted by `w:conservativeTRUE`. Can we detect a difference in treatment effect for conservative vs liberal individuals? For whom is the effect larger?\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1522c8d",
   "metadata": {},
   "source": [
    "<font size=1>\n",
    "Interpret the results above. The coefficient $\\beta_{xw}$ is denoted by `w:conservativeTRUE`. Can we detect a difference in treatment effect for conservative vs liberal individuals? For whom is the effect larger?\n",
    "</font>\n",
    "\n",
    "Sometimes there are many subgroups, leading to multiple hypotheses such as\n",
    "\\begin{equation}\n",
    "(\\#eq:mult-hyp)\n",
    "H_0: \\E[Y(1) - Y(0) \\ | \\  G_i = 1] = \\E[Y(1) - Y(0) \\ | \\  G_i = g]\n",
    "\\qquad\n",
    "\\text{for many values of }g.\n",
    "\\end{equation}\n",
    "\n",
    "In that case, we need to correct for the fact that we are testing for multiple hypotheses, or we will end up with many false positives. The **Bonferroni correction** [(wiki)](https://en.wikipedia.org/wiki/Bonferroni_correction) is a common method for dealing with multiple hypothesis testing, though it is often too conservative to be useful. It is available via the function `p.adjust` from base `R`. The next snippet of code tests whether the treatment effect at each level of `polviews` is different from the treatment effect from `polviews` equals one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3d3d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>unadj_p_value</th>\n",
       "      <th>adj_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.2]</th>\n",
       "      <td>-0.024242</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>3.751982e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.3]</th>\n",
       "      <td>-0.059623</td>\n",
       "      <td>0.027357</td>\n",
       "      <td>2.929996e-02</td>\n",
       "      <td>1.757997e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.4]</th>\n",
       "      <td>-0.134614</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>1.082607e-07</td>\n",
       "      <td>6.495642e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.5]</th>\n",
       "      <td>-0.164915</td>\n",
       "      <td>0.027135</td>\n",
       "      <td>1.220112e-09</td>\n",
       "      <td>7.320672e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.6]</th>\n",
       "      <td>-0.180079</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>5.952137e-11</td>\n",
       "      <td>3.571282e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w:C(polviews)[T.7]</th>\n",
       "      <td>-0.186184</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>1.507202e-06</td>\n",
       "      <td>9.043211e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Estimate  Std.Err.  unadj_p_value   adj_p_value\n",
       "w:C(polviews)[T.2] -0.024242  0.027337   3.751982e-01  1.000000e+00\n",
       "w:C(polviews)[T.3] -0.059623  0.027357   2.929996e-02  1.757997e-01\n",
       "w:C(polviews)[T.4] -0.134614  0.025340   1.082607e-07  6.495642e-07\n",
       "w:C(polviews)[T.5] -0.164915  0.027135   1.220112e-09  7.320672e-09\n",
       "w:C(polviews)[T.6] -0.180079  0.027514   5.952137e-11  3.571282e-10\n",
       "w:C(polviews)[T.7] -0.186184  0.038706   1.507202e-06  9.043211e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only valid in randomized setting.\n",
    "\n",
    "# Example: these groups must be defined prior to collecting the data.\n",
    "group = 'polviews'\n",
    "\n",
    "# Linear regression.\n",
    "fmla = 'y ~ w*C(polviews)'\n",
    "ols = smf.ols(fmla, data=data).fit(cov_type='HC2')\n",
    "\n",
    "# Retrieve the interaction coefficients\n",
    "ols_1 = ols.summary2().tables[1].reset_index()\n",
    "interact = ols_1.loc[ols_1[\"index\"].str.contains(\"w:\")]\n",
    "\n",
    "hypothesis_1 = []\n",
    "for i in list(interact['index']):\n",
    "    hypothesis_1.append(i+str('=0'))\n",
    "hypotheses = hypothesis_1\n",
    "t_test = ols.t_test(hypotheses)\n",
    "# print(t_test.summary(xname=list(interact['index'])))\n",
    "\n",
    "# Retrieve unadjusted p-values and \n",
    "unadj_p_value = list(interact[\"P>|z|\"])\n",
    "p_adjusted = list(multipletests(unadj_p_value, alpha=0.05, method='bonferroni')[1])\n",
    "\n",
    "pd.DataFrame(zip(interact[\"Coef.\"], interact[\"Std.Err.\"], unadj_p_value, p_adjusted),\n",
    "               columns =['Estimate', 'Std.Err.', 'unadj_p_value', 'adj_p_value'],\n",
    "            index = list(interact[\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2f168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which turn a list or vector-like object into a proper two\n",
    "# dimensional column vector\n",
    "\n",
    "def cvec(a):\n",
    "    \"\"\" Turn a list or vector-like object into a proper column vector\n",
    "    Input\n",
    "    a: List or vector-like object, has to be a potential input for np.array()\n",
    "    Output\n",
    "    vec: two dimensional NumPy array, with the first dimension weakly greater\n",
    "         than the second (resulting in a column vector for a vector-like input)\n",
    "    \"\"\"\n",
    "    # Conver input into a two dimensional NumPy array\n",
    "    vec = np.array(a, ndmin=2)\n",
    "\n",
    "    # Check whether the second dimension is strictly greater than the first\n",
    "    # (remembering Python's zero indexing)\n",
    "    if vec.shape[0] < vec.shape[1]:\n",
    "        # If so, transpose the input vector\n",
    "        vec = vec.T\n",
    "\n",
    "    # Return the column vector\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee8a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov(X, e, add_intercept=True, homoskedastic=False):\n",
    "    \"\"\" Calculates OLS variance estimator based on X and residuals\n",
    "    Inputs\n",
    "    X: n by k matrix, RHS variables\n",
    "    e: n by 1 vector or vector-like, residuals from an OLS regression\n",
    "    add_intercept: Boolean, if True, adds an intercept as the first column of X\n",
    "                   (and increases k by one)\n",
    "    Outputs\n",
    "    V_hat: k by k NumPy array, estimated covariance matrix\n",
    "    \"\"\"\n",
    "    # Get the number of observations n and parameters k\n",
    "    n, k = X.shape\n",
    "\n",
    "    # Check whether an intercept needs to be added\n",
    "    if add_intercept:\n",
    "        # If so, add the intercept\n",
    "        X = np.concatenate([np.ones(shape=(n,1)), X], axis=1)\n",
    "\n",
    "        # Don't forget to increase k\n",
    "        k = k + 1\n",
    "\n",
    "    # Make sure the residuals are a proper column vector\n",
    "    e = cvec(e)\n",
    "\n",
    "    # Calculate X'X\n",
    "    XX = X.T @ X\n",
    "\n",
    "    # Calculate its inverse\n",
    "    XXinv = linalg.inv(XX)\n",
    "\n",
    "    # Check whether to use homoskedastic errors\n",
    "    if homoskedastic:\n",
    "        # If so, calculate the homoskedastic variance estimator\n",
    "        V_hat = (1 / (n-k)) * XXinv * (e.T @ e)\n",
    "    else:\n",
    "        # Otherwise, calculate an intermediate object\n",
    "        S = (e @ np.ones(shape=(1,k))) * X\n",
    "\n",
    "        # Then, get the HC0 sandwich estimator\n",
    "        V_hat = (n / (n-k)) * XXinv @ (S.transpose() @ S) @ XXinv\n",
    "\n",
    "    # Return the result\n",
    "    return V_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f876d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to computes adjusted p-values \n",
    "# following the Romano-Wolf method.\n",
    "# For a reference, see http://ftp.iza.org/dp12845.pdf page 8\n",
    "#  t.orig: vector of t-statistics from original model\n",
    "#  t.boot: matrix of t-statistics from bootstrapped models\n",
    "\n",
    "\n",
    "def romano_wolf_correction(t_orig, t_boot):\n",
    "    abs_t_orig = np.absolute(t_orig)\n",
    "    abs_t_boot = np.absolute(t_boot)\n",
    "    abs_t_sorted = sorted(abs_t_orig, key = float, reverse=True)\n",
    "\n",
    "    max_order = (-np.array(abs_t_orig)).argsort()\n",
    "    rev_order = np.argsort(max_order)\n",
    "\n",
    "    M = t_boot.shape[0]\n",
    "    S = t_boot.shape[1]\n",
    "\n",
    "    p_adj = list(np.repeat(0, S))\n",
    "    p_adj[0] = np.mean(pd.DataFrame(abs_t_boot).apply(np.max, axis=1) > abs_t_sorted[0])\n",
    "\n",
    "    for s in range(1,S):\n",
    "        cur_index = max_order[s:S]\n",
    "        p_init = np.mean(\n",
    "            pd.DataFrame(abs_t_boot).T.iloc[cur_index].T.apply(np.max, axis=1) > abs_t_sorted[s])\n",
    "        p_adj[s] = np.max(p_init, p_adj[s])\n",
    "\n",
    "    aux = []\n",
    "    for i in rev_order:\n",
    "        aux.append(p_adj[i])\n",
    "\n",
    "    p_adj = aux\n",
    "    \n",
    "    return(p_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7336da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes adjusted p-values for linear regression (lm) models.\n",
    "#    model: object of lm class (i.e., a linear reg model)\n",
    "#    indices: vector of integers for the coefficients that will be tested\n",
    "#    cov.type: type of standard error (to be passed to sandwich::vcovHC)\n",
    "#    num.boot: number of null bootstrap samples. Increase to stabilize across runs.\n",
    "# Note: results are probabilitistic and may change slightly at every run. \n",
    "#\n",
    "# Adapted from the p_adjust from from the hdm package, written by Philipp Bach.\n",
    "# https://github.com/PhilippBach/hdm_prev/blob/master/R/p_adjust.R\n",
    "\n",
    "def summary_rw_lm(model, indices='', cov_type=\"HC2\", num_boot=10000):\n",
    "    SyncRNG(seed = 123456)  \n",
    "\n",
    "    # OLS without correction\n",
    "\n",
    "    # Grab the original t values.\n",
    "    ols = smf.ols(fmla, data=data).fit()\n",
    "    ols = ols.summary2().tables[1].reset_index()\n",
    "    summary = ols[ols['index'].isin(list(indices[\"index\"]))]\n",
    "    t_orig = summary['t']\n",
    "\n",
    "    # Null resampling.\n",
    "    # This is a trick to speed up bootstrapping linear models.\n",
    "    # Here, we don't really need to re-fit linear regressions, which would be a bit slow.\n",
    "    # We know that betahat ~ N(beta, Sigma), and we have an estimate Sigmahat.\n",
    "    # So we can approximate \"null t-values\" by\n",
    "    #  - Draw beta.boot ~ N(0, Sigma-hat) --- note the 0 here, this is what makes it a *null* t-value.\n",
    "    #  - Compute t.boot = beta.boot / sqrt(diag(Sigma.hat))\n",
    "\n",
    "    ols = smf.ols(fmla, data=data).fit(cov_type = cov_type)\n",
    "    ols_exog = smf.ols(fmla, data=data).exog\n",
    "    ols_res = smf.ols(fmla, data=data).fit().resid\n",
    "    Sigma_hat = get_cov(ols_exog, ols_res, add_intercept=False)\n",
    "\n",
    "    se_orig = pd.Series(np.sqrt(Sigma_hat.diagonal()))\n",
    "\n",
    "    num_coef = len(se_orig)\n",
    "\n",
    "    beta_boot = pd.DataFrame(\n",
    "                np.random.multivariate_normal(\n",
    "                mean=np.repeat(0, num_coef), cov=Sigma_hat, size=num_boot))\n",
    "\n",
    "    t_boot = np.array(beta_boot.apply(lambda row: row / se_orig, axis=1))\n",
    "    t_boot = t_boot.T[(len(ols_1)-len(t_orig)):len(ols_1)].T\n",
    "\n",
    "    p_adj = romano_wolf_correction(t_orig, t_boot)\n",
    "    result = summary[['index','Coef.','Std.Err.','P>|t|']]\n",
    "    result.rename(columns={'P>|t|': 'Orig.p-value'}, inplace=True)\n",
    "    result['Adj. p-value'] = p_adj\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4950cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roberto\\AppData\\Local\\Temp\\ipykernel_7972\\909325009.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Roberto\\AppData\\Local\\Temp\\ipykernel_7972\\909325009.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>Orig.p-value</th>\n",
       "      <th>Adj. p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>w:C(polviews)[T.2]</td>\n",
       "      <td>-0.024242</td>\n",
       "      <td>0.030348</td>\n",
       "      <td>4.244098e-01</td>\n",
       "      <td>0.4270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>w:C(polviews)[T.3]</td>\n",
       "      <td>-0.059623</td>\n",
       "      <td>0.030150</td>\n",
       "      <td>4.798896e-02</td>\n",
       "      <td>0.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>w:C(polviews)[T.4]</td>\n",
       "      <td>-0.134614</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>1.862258e-06</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>w:C(polviews)[T.5]</td>\n",
       "      <td>-0.164915</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>2.481093e-08</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>w:C(polviews)[T.6]</td>\n",
       "      <td>-0.180079</td>\n",
       "      <td>0.029660</td>\n",
       "      <td>1.284150e-09</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>w:C(polviews)[T.7]</td>\n",
       "      <td>-0.186184</td>\n",
       "      <td>0.037904</td>\n",
       "      <td>9.063658e-07</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index     Coef.  Std.Err.  Orig.p-value  Adj. p-value\n",
       "8   w:C(polviews)[T.2] -0.024242  0.030348  4.244098e-01        0.4270\n",
       "9   w:C(polviews)[T.3] -0.059623  0.030150  4.798896e-02        0.0749\n",
       "10  w:C(polviews)[T.4] -0.134614  0.028228  1.862258e-06        0.0000\n",
       "11  w:C(polviews)[T.5] -0.164915  0.029575  2.481093e-08        0.0000\n",
       "12  w:C(polviews)[T.6] -0.180079  0.029660  1.284150e-09        0.0000\n",
       "13  w:C(polviews)[T.7] -0.186184  0.037904  9.063658e-07        0.0000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This linear regression is only valid in a randomized setting.\n",
    "fmla = 'y ~ w*C(polviews)'\n",
    "ols = smf.ols(fmla, data=data).fit()\n",
    "ols = ols.summary2().tables[1].reset_index()\n",
    "interact = ols.loc[ols_1[\"index\"].str.contains(\"w:\")]\n",
    "\n",
    "# Applying the romano-wolf correction.\n",
    "summary_rw_lm(ols, indices=interact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6320d",
   "metadata": {},
   "source": [
    "## Data-driven hypotheses\n",
    "\n",
    "Pre-specifying hypotheses prior to looking at the data is in general good practice to avoid \"p-hacking\" (e.g., slicing the data into different subgroups until a significant result is found). However, valid tests can also be attained if by **sample splitting**: we can use a subset of the sample to find promising subgroups, then test hypotheses about these subgroups in the remaining sample. This kind of sample splitting for hypothesis testing is called **honesty**.\n",
    "\n",
    "### Via causal trees\n",
    "\n",
    "**Causal trees** [(Athey and Imbens)](PNAS, 2016)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf) are an intuitive algorithm that is available in the randomized setting to discover subgroups with different treatment effects.\n",
    "\n",
    "At a high level, the idea is to divide the sample into three subsets (not necessarily of equal size). The `splitting` subset is used to fit a decision tree whose objective is modified to maximize heterogeneity in treatment effect estimates across leaves. The `estimation` subset is then used to produce a valid estimate of the treatment effect at each leaf of the fitted tree. Finally, a `test` subset can be used to validate the tree estimates.\n",
    "\n",
    "The next snippet uses `honest.causalTree` function from the [`causalTree`](https://github.com/susanathey/causalTree) package. For more details, see the [causalTree documentation](https://github.com/susanathey/causalTree/blob/master/briefintro.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae6324c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>age</th>\n",
       "      <th>polviews</th>\n",
       "      <th>income</th>\n",
       "      <th>educ</th>\n",
       "      <th>marital</th>\n",
       "      <th>sex</th>\n",
       "      <th>conservative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28648</th>\n",
       "      <td>36497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28649</th>\n",
       "      <td>36498</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28650</th>\n",
       "      <td>36499</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28651</th>\n",
       "      <td>36500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28652</th>\n",
       "      <td>36501</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28653 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X  y  w  age  polviews  income  educ  marital  sex  conservative\n",
       "0          1  0  0   28         4      11    14        5    1             0\n",
       "1          2  1  0   54         6      12    16        2    2             0\n",
       "2          3  1  0   44         2      12    16        5    2             1\n",
       "3          6  0  0   47         1       5    10        4    1             1\n",
       "4          7  0  1   19         4       9    10        5    2             0\n",
       "...      ... .. ..  ...       ...     ...   ...      ...  ...           ...\n",
       "28648  36497  0  0   62         5      12    16        1    1             0\n",
       "28649  36498  1  0   66         7       9    12        2    2             0\n",
       "28650  36499  0  1   54         3      11    12        4    2             1\n",
       "28651  36500  0  0   57         3       6    16        3    2             1\n",
       "28652  36501  1  0   30         4      12    14        5    1             0\n",
       "\n",
       "[28653 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed87e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['age','polviews', 'income','educ','marital','sex']]\n",
    "y = data['y']\n",
    "treatment = data['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e043973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roberto\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3723: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "C:\\Users\\Roberto\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "columns = X.columns # get varaible's names in a list\n",
    "\n",
    "# From DataFrame to array \n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "treatment = treatment.values\n",
    "\n",
    "# CL-honest\n",
    "\n",
    "cthl = CausalTree(honest=True, min_size=1, split_size=0.33)\n",
    "cthl.fit(X, y, treatment)\n",
    "cthl.prune()\n",
    "\n",
    "# plot\n",
    "cthl.plot_tree(features=columns, filename=\"bin_tree_honest_1\", show_effect=True, alpha = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8efaa3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test sample\n",
    "\n",
    "train_x, val_x, train_y, val_y, train_t, val_t = train_test_split(X, y, treatment, random_state=724, shuffle=True,\n",
    "                                                                          test_size=0.33)\n",
    "# get honest/estimation portion\n",
    "train_x, est_x, train_y, est_y, train_t, est_t = train_test_split(train_x, train_y, train_t, shuffle=True,\n",
    "                                                                          random_state=724, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79efb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "cthl_predict = cthl.predict(est_x)  # Predict tau \n",
    "num_leaves = len(np.unique(cthl_predict)) # number of leaves \n",
    "\n",
    "labels = [i for i in range(1,len(np.unique(cthl_predict)) + 1 ) ] # label by each leaf \n",
    "\n",
    "# Prediction grouped by each leaf\n",
    "\n",
    "predict = pd.DataFrame({\"predict\": cthl_predict})\n",
    "predict['leaves'] = pd.Categorical(predict.predict)\n",
    "predict['leaves'] = predict['leaves'].cat.rename_categories(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d428f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>-0.406130</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>-0.433803</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>-0.250101</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9598</th>\n",
       "      <td>-0.369901</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9599 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predict leaves\n",
       "0    -0.369901      7\n",
       "1    -0.369901      7\n",
       "2    -0.369901      7\n",
       "3    -0.369901      7\n",
       "4    -0.369901      7\n",
       "...        ...    ...\n",
       "9594 -0.406130      5\n",
       "9595 -0.433803      3\n",
       "9596 -0.250101     10\n",
       "9597 -0.369901      7\n",
       "9598 -0.369901      7\n",
       "\n",
       "[9599 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c2b6e",
   "metadata": {},
   "source": [
    "## Causal Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6291a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87290606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import econml\n",
    "# Main imports\n",
    "from econml.orf import DMLOrthoForest, DROrthoForest\n",
    "from econml.dml import CausalForestDML\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso, WeightedLassoCV\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "# Helper imports\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from econml.grf import RegressionForest\n",
    "%matplotlib inline\n",
    "import patsy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data to fit a causal forest\n",
    "\n",
    "fmla = '0+age+polviews+income+educ+marital+sex'\n",
    "desc = patsy.ModelDesc.from_formula(fmla)\n",
    "desc.describe()\n",
    "matrix = patsy.dmatrix(fmla, data, return_type = \"dataframe\")\n",
    "\n",
    "T = data.loc[ : ,\"w\"]\n",
    "Y = data.loc[ : ,\"y\"]\n",
    "X = matrix\n",
    "W = None \n",
    "\n",
    "# Estimate a causal forest.\n",
    "est2 = CausalForestDML(model_t=RegressionForest(),\n",
    "                       model_y=RegressionForest(),\n",
    "                       n_estimators=200, min_samples_leaf=5,\n",
    "                       max_depth=50,\n",
    "                       verbose=0, random_state=123)\n",
    "est2.tune(Y, T, X=X, W=W)\n",
    "est2.fit(Y, T, X=X, W=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residuals  and propensity \n",
    "residuals = est2.fit(Y, T, X=X, W=W, cache_values=True).residuals_\n",
    "T_res = residuals[1]\n",
    "e_hat = T - T_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20108136",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prop = pd.DataFrame({\"p_score\":e_hat, \"Treatment\":T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137440c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=Prop, x=\"p_score\", hue=\"Treatment\", bins=40, alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82e761",
   "metadata": {},
   "source": [
    "Having fit a non-parametric method such as a causal forest, a researcher may (incorrectly) start by looking at the distribution of its predictions of the treatment effect. One might be tempted to think: \"if the histogram is concentrated at a point, then there is no heterogeneity; if the histogram is spread out, then our estimator has found interesting heterogeneity.\" However, this may be false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_hat = est2.effect(X=X) # tau(X) estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9a8c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do not use this for assessing heterogeneity. See text above.\n",
    "sns.displot( tau_hat, stat=\"density\", bins = 10)\n",
    "plt.title(\"CATE estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be871910",
   "metadata": {},
   "source": [
    "If the histogram is concentrated at a point, we may simply be underpowered: our method was not able to detect any heterogeneity, but maybe it would detect it if we had more data. If the histogram is spread out, we may be overfitting: our model is producing very noisy estimates $\\widehat{\\tau}(x)$, but in fact the true  $\\tau(x)$ can be much smoother as a function of $x$.\n",
    "\n",
    "The `grf` package also produces a measure of variable importance that indicates how often a variable was used in a tree split. Again, much like the histogram above, this can be a rough diagnostic, but it should not be interpreted as indicating that, for example, variable with low importance is not related to heterogeneity. The reasoning is the same as the one presented in the causal trees section: if two covariates are highly correlated, the trees might split on one covariate but not the other, even though both (or maybe neither) are relevant in the true data-generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e77368",
   "metadata": {},
   "outputs": [],
   "source": [
    "est2.feature_importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146daceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({\"covariaties\" : list(X.columns), \"values\" : est2.feature_importances()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fea211",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.sort_values('values', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea45d1",
   "metadata": {},
   "source": [
    "#### Data-driven subgroups\n",
    "\n",
    "Just as with causal trees, we can use causal forests to divide our observations into subgroups. In place of leaves, we'll rank observation into (say) quintiles according to their estimated CATE prediction; see, e.g., [Chernozhukov, Demirer, Duflo, Fernández-Val (2020)](https://arxiv.org/abs/1712.04802) for similar ideas.\n",
    "\n",
    "There's a subtle but important point that needs to be addressed here. As we have mentioned before, when predicting the conditional average treatment effect $\\tau(X_i)$ for observation $i$ we should in general avoid using a model that was fitted using observation $i$. This sort of sample splitting (which we called **honesty** above) is one of the required ingredients to get unbiased estimates of the CATE using the methods described here. However, when ranking estimates of two observations $i$ and $j$, we need something a little stronger: we must ensure that the model was not fit using _either_ $i$ _or_ $j$'s data. \n",
    "\n",
    "One way of overcoming this obstacle is simple. First, divide the data into $K$ folds (subsets). Then, cycle through the folds, fitting a CATE model on $K-1$ folds. Next, for each held-out fold, _separately_ rank the unseen observations into $Q$ groups based on their prediction  (i.e., if $Q=5$, then we rank observations by estimated CATE into \"top quintile\", \"second quintile\", and so on). After concatenating the independent rankings together, we can study the differences in observations in each rank-group. \n",
    "\n",
    "[This gist](https://gist.github.com/halflearned/bea4e5137c0c81fd18a75f682da466c8) computes the above for `grf`, and it should not be hard to modify it so as to replace forests by any other non-parametric method. However, for `grf` specifically, there's a small trick that allows us to obtain a valid ranking: we can pass a vector of fold indices to the argument `clusters` and rank observations within each fold. This works because estimates for each fold (\"cluster\")   trees are computed using trees that were not fit using observations from that fold. Here's how to do it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6da6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid randomized data and observational data with unconfoundedness+overlap.\n",
    "# Note: read the comments below carefully. \n",
    "# In randomized settings, do not estimate forest.e and e.hat; use known assignment probs.\n",
    "\n",
    "#\n",
    "# Prepare dataset\n",
    "fmla = '0+age+polviews+income+educ+marital+sex'\n",
    "desc = patsy.ModelDesc.from_formula(fmla)\n",
    "matrix = patsy.dmatrix(fmla, data, return_type = \"dataframe\")\n",
    "\n",
    "T = data.loc[ : ,\"w\"]\n",
    "Y = data.loc[ : ,\"y\"]\n",
    "X = matrix\n",
    "W = None \n",
    "\n",
    "\n",
    "# Number of rankings that the predictions will be ranking on \n",
    "# (e.g., 2 for above/below median estimated CATE, 5 for estimated CATE quintiles, etc.)\n",
    "num_rankings = 5  \n",
    "\n",
    "# Prepare for data.splitting\n",
    "# Assign a fold number to each observation.\n",
    "# The argument 'clusters' in the next step will mimick K-fold cross-fitting.\n",
    "num_folds = 10\n",
    "\n",
    "#folds = sort(seq(n) %% num_folds) + 1\n",
    "\n",
    "# Estimate a causal forest.\n",
    "forest = CausalForestDML(model_t=RegressionForest(),\n",
    "                       model_y=RegressionForest(),\n",
    "                       n_estimators=2000, min_samples_leaf=5,\n",
    "                       max_depth=50,\n",
    "                       verbose=0, random_state=123, cv = num_folds)\n",
    "forest.tune(Y, T, X=X, W=W)\n",
    "forest.fit(Y, T, X=X, W=W)\n",
    "\n",
    "tau_hat = forest.effect(X=X) # tau(X) estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cdefd6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEEAAAAQCAYAAABJJRIXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAABJ0AAASdAHeZh94AAADoklEQVR4nO3XW4iVVRQH8N+Y0EUrI8shiMAo0qTUQiqlGz10sdKo6EHJIBUqimroRrVcRaBFotZDWpZZvoSkD1pSkmBlFGTig5GaZBfSSgu6aFJOD98+cTxzzsw481h/+Fhn73Xbe5211167rbOz038dA+sHmXkjLsFonItjsSwiprQykJnX4B6MxIn4Hp9ibkR81NMCMnMqlpbh9Ih4qYnMHJyPMzEU+7ATK/F8ROypk52GV3pwezAijqgNBjQwH8VdqiB814sNzMEqjMUazMdGXI8PM7Nl8Ir+qXgOv/Xg6l4MwrvFxzL8hVnYXOzUsAnZ4nuvyLxdb/yQTCjOvsV2VUas62YD7ejAbpwTET/U8S4rDp/A6y3021T/2B68WWy1wnERsb+JjafwCB7GHRARm1SBaOazlpmL6ucPyYSIWBcR2yKiN4XitKL/cX0AanbwK07qRv9uXI7b8Ht3jpoFoOCNQs/oabGZOQoXqDJ8dT2v8TgcDrbhAMZl5tAGhxer6snaFgsagdmYHxHr+7GGawvd3AvZmYUujoi/6xmNx6HXiIi9mfkg5mJLZq5UpfbpuE51fmc26mXmQLyGr1Wp3GtkZgcG43hVoZygCsDsHvSOxhQcRJfC2+cgQETMy8yv8DKm17G2Y0njMSl4HGMwISL2HabLDgyrG6/BtIj4sQe9mzEEqyPim0Zmf46DzHwAy7FElQGDcB52YFlmPt0gP0717z/bm+uzERHRHhFtaMcNGI7PMnNsD6ozCl3YjNnnTMjMSzEHKyLivjrWxsycjK24PzNfiIgddcdgKx7rq1+IiN1YkZkbi72lGNVinSNxkerWe6uZTH8yYWKhXa7RiPgDnxT7Y8r0YFWzMwL7M7Oz9iGKzItlbl5vFhARO7EFZzcW5zq0LIg19KcmHFloq2uwNn+g0D+xuIXsWFWwPsAXOJyjckqhXTaYmUdhqqogtvLdryC8r+ouZ2Tmwoj4t8PMzKswHvuxAUoRvL2ZocycpQrCq41tc2aehV8iYlfD/AA8iZOxISJ+bmL6JpyAVc0KYg2Nb4dJmFSG7YVemJlLyu+fIqLW2S1X9QFX4PPMXIFdqnSfiDY8VN/X9xFX4pnMXI8vVdfwMFVHO7z4nN5Ct1YQF7Xgo2smjMatDXPDy0f1aOmAiDiYmVfjTtyCyTgGe1UFaEFEvNOd815irWoT41WPuiGqDnOrqtAuiIi9jUqlIZugm4JYQ9v/T2n+AazrS99zqRkBAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle 18437$"
      ],
      "text/plain": [
       "18437"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(tau_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a854738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid randomized data and observational data with unconfoundedness+overlap.\n",
    "# Note: read the comments below carefully. \n",
    "# In randomized settings, do not estimate forest.e and e.hat; use known assignment probs.\n",
    "\n",
    "# Prepare dataset\n",
    "fmla <- formula(paste0(\"~ 0 + \", paste0(covariates, collapse=\"+\")))\n",
    "X <- model.matrix(fmla, data)\n",
    "W <- data[,treatment]\n",
    "Y <- data[,outcome]\n",
    "\n",
    "# Number of rankings that the predictions will be ranking on \n",
    "# (e.g., 2 for above/below median estimated CATE, 5 for estimated CATE quintiles, etc.)\n",
    "num.rankings <- 5  \n",
    "\n",
    "# Prepare for data.splitting\n",
    "# Assign a fold number to each observation.\n",
    "# The argument 'clusters' in the next step will mimick K-fold cross-fitting.\n",
    "num.folds <- 10\n",
    "folds <- sort(seq(n) %% num.folds) + 1\n",
    "\n",
    "# Comment or uncomment depending on your setting.\n",
    "# Observational setting with unconfoundedness+overlap (unknown assignment probs):\n",
    "# forest <- causal_forest(X, Y, W, clusters = folds)\n",
    "# Randomized settings with fixed and known probabilities (here: 0.5).\n",
    "forest <- causal_forest(X, Y, W, W.hat=.5, clusters = folds)\n",
    "\n",
    "# Retrieve out-of-bag predictions.\n",
    "# Predictions for observation in fold k will be computed using \n",
    "# trees that were not trained using observations for that fold.\n",
    "tau.hat <- predict(forest)$predictions\n",
    "\n",
    "# Rank observations *within each fold* into quintiles according to their CATE predictions.\n",
    "ranking <- rep(NA, n)\n",
    "for (fold in seq(num.folds)) {\n",
    "  tau.hat.quantiles <- quantile(tau.hat[folds == fold], probs = seq(0, 1, by=1/num.rankings))\n",
    "  ranking[folds == fold] <- cut(tau.hat[folds == fold], tau.hat.quantiles, include.lowest=TRUE,labels=seq(num.rankings))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afdc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94a903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b85ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfc826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e42071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405934fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9138ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884ad46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec9d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
