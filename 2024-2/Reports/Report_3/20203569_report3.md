## Report 3: Estimation and Inference of Heterogeneous Tratment Effects using Random Forests 


### Student: Charles SÃ¡nchez Salas 

The articles' aim is estimating asymptotically normal and unbiased heterogeneous treatment effects with valid confidence intervals using Random Forests, a widely used method in Machine Learning, in order to fix the problem that classical inference methods- nearest-neighbor mathing, kernel methods, series estimation- have to estimate heterogeneous treatment effects in contexts of high-dimensionality (big number of covariates). 

In order to build this *causal forest*, the authors clearly stablish the theoretical conditions this new method should have to estimate average treatment effects within the leaps: consistency (when the conditional mean functions are both Lipschitz continuous), centered asymptotic normality (the sup-sample must scale within the appropiate bounds), and honesty (when the three only uses the response variable to estimate the within-leaf treatment effect). On the other hand, in order to use random forests to generate valid statistical inference, the authors design an asymptotic theory for random forests. Finally, the authors test the efficiency of the method using a pre-designed experiment and comparing the results with classical inference methods. The results show that *causal forest* is more powerful than the previous methods to estimate validate heterogeneous treatment effects, especially in the presence of irrelevant covariates. 

A limitation of the study is that the authors manually chose whether to use double-sample forests or propensity forests (two different algorithms used to generate honesty trees) during the experimentation. To fix this limitation, the authors suggests that future works should try to design splitting rules that automatically choose which characteristic of the training data to split on . 

In general, the next steps following after this paper is to identify methods that can produce valid statistical effects in more challenging contexts (e.g., with small samples or a large number of covariates (high-dimensionality))