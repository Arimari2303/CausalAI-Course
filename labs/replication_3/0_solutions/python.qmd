
## Ortogonal Learning

```{python}
# !pip install multiprocess
# !git clone https://github.com/maxhuppertz/hdmpy.git
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import hdmpy as hdm


def simulate(n=0):
    n = 1000
    p = 100
    beta = (1 / (np.arange(1, p + 1) ** 2)).reshape(p, 1)
    gamma = (1 / (np.arange(1, p + 1) ** 2)).reshape(p, 1)

    mean = 0
    sd = 1
    X = np.random.normal(mean, sd, n * p).reshape(n, p)

    D = (X @ gamma) + np.random.normal(mean, sd, n).reshape(n, 1) / 4

    Y = 10 * D + (X @ beta) + np.random.normal(mean, sd, n).reshape(n, 1)

    r_lasso_estimation = hdm.rlasso(np.concatenate((D, X), axis=1), Y, post=True)

    coef_array = r_lasso_estimation.est["coefficients"].iloc[2:, :].to_numpy()

    SX_IDs = np.where(coef_array != 0)[0]

    if sum(SX_IDs) == 0:
        Naive = (
            sm.OLS(Y, sm.add_constant(D)).fit().summary2().tables[1].round(3).iloc[1, 0]
        )
    elif sum(SX_IDs) > 0:
        X_D = np.concatenate((D, X[:, SX_IDs]), axis=1)
        Naive = (
            sm.OLS(Y, sm.add_constant(X_D))
            .fit()
            .summary2()
            .tables[1]
            .round(3)
            .iloc[1, 0]
        )

    resY = hdm.rlasso(X, Y, post=False).est["residuals"]
    resD = hdm.rlasso(X, D, post=False).est["residuals"]
    Ortho = (
        sm.OLS(resY, sm.add_constant(resD))
        .fit()
        .summary2()
        .tables[1]
        .round(3)
        .iloc[1, 0]
    )
    return Naive, Ortho


def simulate_and_plot(B1):
    np.random.seed(0)
    Naive = np.zeros(B1)
    Orthogonal = np.zeros(B1)

    for i in range(0, B1):
        Naive[i], Orthogonal[i] = simulate()

    Orto_breaks = [
        -1.2,
        -1,
        -0.8,
        -0.6,
        -0.4,
        -0.2,
        0,
        0.2,
        0.4,
        0.6,
        0.8,
        1,
        1.2,
        1.4,
        1.6,
        1.8,
        2,
    ]
    Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]

    fig, axs = plt.subplots(1, 2, sharex=True, tight_layout=True)

    axs[0].hist(
        Orthogonal - 10, range=(-2, 2), density=True, bins=Orto_breaks, color="blue"
    )
    axs[1].hist(Naive - 10, range=(-2, 2), density=True, bins=Naive_breaks, color="red")

    axs[0].set_title(f"Orthogonal, {B1} trials")
    axs[1].set_title(f"Naive {B1} trials")

    axs[0].set_xlabel("Orthogonal")
    axs[1].set_xlabel("Naive")

    plt.show()
    return


simulate_and_plot(10)
```


```{python}
simulate_and_plot(100)
simulate_and_plot(1000)
simulate_and_plot(10000)
```


```{python}
import time
s = time.time()
for i in range(2):
    simulate()
time.time()- s
```

```{python}
s = time.time()
with multiprocessing.Pool(4) as parallel:
    results = parallel.map(simulate, range(2))
time.time() - s
```


## Doble Lasso

```{python}
import pandas as pd
import numpy as np
from sklearn.linear_model import LassoCV, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv("https://raw.githubusercontent.com/gsbDBI/ExperimentData/97a11199ba559f01c7b3803a1493ffa08631732e/School/bruhn2016.csv")
data.dropna(inplace=True)

data.columns = [col.replace('.', '_') for col in data.columns]

data.columns


```

```{python}
out_come = "outcome_test_score"
_d = 'treatment'
cols = np.array(data.columns)
xall = cols[np.isin(cols, [out_come, _d], invert=True)]
np.random.seed(12)
i_xall = np.random.choice(len(xall), 3, replace=False)
x_selected = xall[i_xall]
x_selected

```

```{python}
X = data[np.append(x_selected, _d)]
X = sm.add_constant(X)
```

### OLS

```{python}
def ols_table(y, X, method = 'ols'):
    model = sm.OLS(y, X).fit()
    result = model.conf_int()
    result['params'] = model.params
    result.columns = ['l', 'u', 'coef']
    result['method'] = method
    # result
    return result
ols = ols_table(data[out_come], X)
ols
```


### Doble Lasso

```{python}
def residual_lasso(X, y, cv_n = 10):
    model = LassoCV(cv = cv_n).fit(X, y)
    residual = y - model.predict(X)
    return residual
# x_selected
```

```{python}
X1 = data[x_selected]
y = data[out_come]
d = data[_d]
lasso_cv_r_y = residual_lasso(X1, y)
lasso_cv_r_D = residual_lasso(X1, d)
d_lasso_result = ols_table(lasso_cv_r_y, lasso_cv_r_D, method = 'Doble Lasso')
d_lasso_result
```


### Doble Lasso - theorical lambda

```{python}
def t_residual_lasso(X, y, theorical_lambda = .5):
    model = Lasso(alpha = theorical_lambda).fit(X, y)
    residual = y - model.predict(X)
    return residual

t_lass_r_y = t_residual_lasso(X1, y)
t_lass_r_d = t_residual_lasso(X1, d)
t_lass_result = ols_table(t_lass_r_y, t_lass_r_d, method = "Doble Lasso - Theorical lambda")
t_lass_result
```

### Doble Lasso - method partilling out


```{python}
import hdmpy as hdm

d_lasso = hdm.rlassoEffect(x=X1, y=y, d=d, method="partialling out")
coef = d_lasso["alpha"]
se = d_lasso["se"]
_l = coef - 1.96 * se
_u = coef + 1.96 * se
d_lass_m = pd.DataFrame(
    {"coef": [coef], "l": _l, "u": _u, "method": "Lasso - Partilling out"}
)

d_lass_m.index = ['treatment']
d_lass_m
```

### Coef Plot

```{python}
df = pd.concat((ols, d_lasso_result, t_lass_result, d_lass_m)).query("index == 'treatment'")
df
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.pointplot(x="method", y="coef", data=df, join=False, color="black")


plt.errorbar(
    x=range(len(df)),
    y=df["coef"],
    yerr=[df["coef"] - df["l"], df["u"] - df["coef"]],
    fmt="none",
    capsize=5,
    ecolor="gray",
)


plt.xlabel("")
plt.ylabel("Coef")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()


plt.show()
```