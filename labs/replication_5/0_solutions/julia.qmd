---
title: "Lab 5 - Julia"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

# Data Analysis

## Table Presentation

```{julia}
using DataFrames
using CSV
using Statistics
using StatsPlots

# Leer los datos
url = "https://raw.githubusercontent.com/alexanderquispe/CausalAI-Course/main/data/processed_esti.csv"
data = CSV.read(download(url), DataFrame);

```

```{julia}
data1 = data[:, [:y, :w, :gender_female, :gender_male, :gender_transgender, :age, :imd_decile]]
data1 = stack(data1, [:gender_female, :gender_male, :gender_transgender],
    variable_name=:gender, value_name=:value)
data1 = filter(row -> row.value > 0, data1)
# data1.gender = replace.(split.(string.(data1.gender), "_"), "gender" => "")
data1 = select(data1, Not(:value))


summary = combine(groupby(data1, [:w, :gender]),
    :y => mean => :mean_y,
    :y => std => :sd_y,
    :age => mean => :mean_age,
    :age => std => :sd_age
)
summary.w = replace(summary.w, 0 => "Control", 1 => "Treatment")
summary.gender = replace(summary.gender, "gender_female" => "Female", "gender_male" => "Male", "gender_transgender" => "Transgender")
summary
```


# Linear Regression Analysis

```{julia}
using Distributions, LinearAlgebra, DataFrames, GLM

function ols_table(data, m="ols", fmla=@formula(y ~ 0 + treatment))
    model = lm(fmla, data)
    coefs = coef(model)
    var = coefnames(model)
    ci = confint(model)
    result = DataFrame(
        l=ci[:, 1],
        u=ci[:, 2],
        coef=coefs,
        method=m,
        term=var
    )
    return result
end
```

## LM 1

```{julia}
lm1 = ols_table(data, "Simple", @formula(y ~ w))
```

## LM 2

```{julia}

lm2 = ols_table(data, "Multiple", @formula(y ~ w + age + imd_decile))

```

## Double Lasso

```{julia}
using Lasso

y_fml = @formula(y ~ gender_female + gender_male + age + imd_decile)
d_fml = @formula(w ~ gender_female + gender_male + age + imd_decile)

function residual_lass(fml, data, col_real="y")

    lasso_modl_ = fit(LassoModel, fml, data)
    real_y = data[!, Symbol(col_real)]
    res = real_y - predict(lasso_modl_)
    return res
end


y_r = residual_lass(y_fml, data)
d_r = residual_lass(d_fml, data, "w")
df2 = DataFrame(y=y_r, w=d_r)

lm3 = ols_table(df2, "Lasso", @formula(y ~ w))
```

## Results

```{julia}
using Plots
combined_results = vcat(lm1, lm2, lm3)
filtered_results = filter(
    row -> row[:term] == "w", combined_results
)
filtered_results
```

```{julia}
plot()

for model in unique(filtered_results.method)
    ref = filter(row -> row[:method] == model, filtered_results)
    x = [model, model]
    y = [ref[1, :l], ref[1, :u]]

    # Añadir puntos y líneas al gráfico
    scatter!([model], [ref[1, :coef]], label="")
    plot!(x, y, label="")
end
xlabel!("Model")
ylabel!("Coefficient")
title!("Value of treatment")
```


# Non Linear Methods




```{julia}
function stratified_split(data::DataFrame, ycol::Symbol, train_ratio::Float64)
    unique_vals = unique(data[!, ycol])
    train_indices = Vector{Int}()
    test_indices = Vector{Int}()

    for val in unique_vals
        idxs = findall(data[!, ycol] .== val)
        n_train = ceil(Int, train_ratio * length(idxs))
        train_idxs = sample(idxs, n_train, replace=false)
        test_idxs = setdiff(idxs, train_idxs)

        append!(train_indices, train_idxs)
        append!(test_indices, test_idxs)
    end

    return (train=train_indices, test=test_indices)
end

split_indices = stratified_split(data, :y, 0.8)
data_train = data[split_indices.train, :]
data_test = data[split_indices.test, :]

y_train = data_train[!, :y]
d_train = data_train[!, :w]
x_train = select(data_train, Not([:y, :w]))

y_test = data_test[!, :y]
d_test = data_test[!, :w]
x_test = select(data_test, Not([:y, :w]));

```


```{julia}

function lm_yd(y, w, m="ols", fmla=@formula(y ~ w))

    data = DataFrame(y=y, w=w)
    model = lm(fmla, data)
    coefs = coef(model)
    var = coefnames(model)
    ci = confint(model)
    result = DataFrame(
        l=ci[:, 1],
        u=ci[:, 2],
        coef=coefs,
        method=m,
        term=var
    )
    return result
end
# lm_yd(y_train, d_train)
```


## Lasso

```{julia}

function residual_lass_ref(fml, data_train, data_test, y_real)

    lasso_modl_ = fit(LassoModel, fml, data_train)
    res = y_real - predict(lasso_modl_, data_test)
    return res
end

y_r1 = residual_lass_ref(y_fml, data_train, data_test, y_test)
d_r1 = residual_lass_ref(d_fml, data_train, data_test, d_test)


l1 = lm_yd(y_r1, d_r1, "Lasso")

```

## Regression Trees

```{julia}
using DecisionTree

mx_train = Matrix(x_train)
mx_test = Matrix(x_test)

model1 = build_tree(y_train, mx_train)
res1 = y_test - apply_tree(model1, mx_test)

model2 = build_tree(d_train, mx_train)
res2 = d_test - apply_tree(model1, mx_test)

l2 = lm_yd(res1, res2, "Reg. Trees")

```


## Boosting Trees

```{julia}
# using XGBoost

# bst1 = xgboost((mx_train, y_train), num_round=5, max_depth=6, objective="reg:squarederror")
# predict(bst1, DMatrix(mx_train))

```

## Regresssion Forest


```{julia}
model1 = build_forest(y_train, mx_train)
res1 = y_test - apply_forest(model1, mx_test)

model2 = build_forest(d_train, mx_train)
res2 = d_test - apply_forest(model1, mx_test)

l4 = lm_yd(res1, res2, "Reg. Forest")

```

## Results

```{julia}

results = vcat(l1, l2, l4)

filtered_results = filter(
    row -> row[:term] == "w", results
)
filtered_results
```

### Plot

```{julia}
plot()

for model in unique(results.method)
    ref = filter(row -> row[:method] == model, results)
    x = [model, model]
    y = [ref[1, :l], ref[1, :u]]

    # Añadir puntos y líneas al gráfico
    scatter!([model], [ref[1, :coef]], label="")
    plot!(x, y, label="")
end
xlabel!("Model")
ylabel!("Coefficient")
title!("Value of treatment")
```
