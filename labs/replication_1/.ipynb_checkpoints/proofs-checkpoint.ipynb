{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77daac73-2b26-4f4f-9251-fdfcf48e5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Hello world\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f0f18-a396-4258-bde5-79363b763f89",
   "metadata": {},
   "source": [
    "##### Theorem 1: FWL\r\n",
    " Let $\\boldsymbol{y} = D\\boldsymbol{\\beta}_1 + W\\boldsymbol{\\beta}_2 + \\boldsymbol{\\mu}$ and let $\\boldsymbol{\\epsilon^Y}$, $\\boldsymbol{\\epsilon^D}$ be $\\boldsymbol{y}$ and $D$ residualized from $W$ respectively. $\\boldsymbol{\\hat{\\beta}}_1$ can be estimated by running OLS of $\\boldsymbol{\\epsilon^Y}$ against $\\boldsymbol{\\epsilon^D}$. \r\n",
    "\r\n",
    "<u>Proof</u>: \r\n",
    "From the original regression, the objective function is the following.\r\n",
    "$$\r\n",
    "\\begin{align*}\r\n",
    "    \\text{min  }\\boldsymbol{e}'\\boldsymbol{e} &= (\\boldsymbol{y} - D\\boldsymbol{\\hat{\\beta}}_1 - W\\boldsymbol{\\hat{\\beta}}_2)'(\\boldsymbol{y} - D\\boldsymbol{\\hat{\\beta}}_1 - W\\boldsymbol{\\hat{\\beta}}_2) \\\\\r\n",
    "    &= \\boldsymbol{y}'\\boldsymbol{y} - 2\\boldsymbol{y}'D\\boldsymbol{\\hat{\\beta}}_1 - 2\\boldsymbol{y}'W\\boldsymbol{\\hat{\\beta}}_2 + 2\\boldsymbol{\\hat{\\beta}}_1'D'W\\boldsymbol{\\hat{\\beta}}_2 + \\boldsymbol{\\hat{\\beta}}_1'D'D\\boldsymbol{\\hat{\\beta}}_1 + \\boldsymbol{\\hat{\\beta}}_2'W'W\\boldsymbol{\\hat{\\beta}}_2\r\n",
    "\\end{align*}\r\n",
    "$$\r\n",
    "The first order conditions are given by the following system\r\n",
    "$$\r\n",
    "\\begin{bmatrix}\r\n",
    "D'D & D'W \\\\\r\n",
    "W'D & W'W \\\\\r\n",
    "\\end{bmatrix}\r\n",
    "\\begin{bmatrix}\r\n",
    "\\boldsymbol{\\hat{\\beta}}_1 \\\\\r\n",
    "\\boldsymbol{\\hat{\\beta}}_2 \\\\\r\n",
    "\\end{bmatrix}\r\n",
    "=\r\n",
    "\\begin{bmatrix}\r\n",
    "D'\\boldsymbol{y} \\\\\r\n",
    "W'\\boldsymbol{y}\r\n",
    "\\end{bmatrix}\r\n",
    "$$\r\n",
    "Lastly, solving for $\\boldsymbol{\\hat{\\beta}}_2$ in the 2nd equation and replacing in the 1st yields the desired result. \r\n",
    "$$\r\n",
    "\\begin{align*}\r\n",
    "    \\boldsymbol{\\hat{\\beta}}_2 &= (W'W)^{-1}[W'\\boldsymbol{y} - W'D\\boldsymbol{\\hat{\\beta}}_1] \\\\\r\n",
    "    &= (W'W)^{-1}W'[\\boldsymbol{y} - D\\boldsymbol{\\hat{\\beta}}_1]\r\n",
    "\\end{align*}\r\n",
    "$$\r\n",
    "In the 1st equation,\r\n",
    "$$D'D\\boldsymbol{\\hat{\\beta}}_1 + D'W\\boldsymbol{\\hat{\\beta}}_2 = D'\\boldsymbol{y}$$\r\n",
    "$$D'D\\boldsymbol{\\hat{\\beta}}_1 + D'W(W'W)^{-1}W'[\\boldsymbol{y} - D\\boldsymbol{\\hat{\\beta}}_1] = D'\\boldsymbol{y}$$\r\n",
    "$$D'D\\boldsymbol{\\hat{\\beta}}_1 + D'P_W[\\boldsymbol{y} - D\\boldsymbol{\\hat{\\beta}}_1] = D'\\boldsymbol{y}$$\r\n",
    "$$D'D\\boldsymbol{\\hat{\\beta}}_1 - D'P_WD\\boldsymbol{\\hat{\\beta}}_1 = D'\\boldsymbol{y} - D'P_W\\boldsymbol{y}$$\r\n",
    "$$D'(I-P_W)D\\boldsymbol{\\hat{\\beta}}_1 = D'(I-P_W)\\boldsymbol{y}$$\r\n",
    "$$D'M_WD\\boldsymbol{\\hat{\\beta}}_1 = D'M_W\\boldsymbol{y}$$\r\n",
    "$$D'M_W'M_WD\\boldsymbol{\\hat{\\beta}}_1 = D'M_W'M_W\\boldsymbol{y}$$\r\n",
    "$$\\boldsymbol{\\hat{\\beta}}_1 = (D'M_W'M_WD)^{-1}D'M_W'M_W\\boldsymbol{y}$$\r\n",
    "$$\\boldsymbol{\\hat{\\beta}}_1 = ({\\epsilon^{D}}^{'}{\\epsilon^{D}})^{-1}\\epsilon^D\\boldsymbol{\\epsilon^Y}$$\r\n",
    "where $P_W$ is $W$'s projection matrix and $M_W$ its residual-maker matrix. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a35051-b44f-41a9-92a6-6711ae6e90cd",
   "metadata": {},
   "source": [
    "##### Theorem 2: CEF minimizes MSE\n",
    "Let $Y=m(X)+$ where $m(X)=E[Y|X]$ is the CEF and g(X) any other function. The CEF $m(x)$ minimizes E[(Y-g(X))].\n",
    "\n",
    "<u>Proof</u>: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    E[(Y-g(X))^2] &= E[(Y-m(X)+m(X)-g(X))^2] \\\\\n",
    "    &= E[(Y-m(X))^2] + E[(m(X)-g(X))^2] + 2E[Y-m(X)]E[m(X)-g(X)]\n",
    "\\end{align*}\n",
    "$$\n",
    "By the Law of Iterated Expectations, the last term is equal to zero since\n",
    "$$\n",
    "\\begin{align*}\n",
    "    E[Y-m(X)]E[m(X)-g(X)] &= E[E[Y-m(X)]E[m(X)-g(X)] |X] \\\\ \n",
    "    &= E[(E[Y|X] - m(X) )( m(X) - g(X) ) |X] \\\\\n",
    "    &= E[( 0 )( m(X) - g(X) ) |X] = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "So $E[(Y-g(X))^2] = E[(Y-m(X))^2] + E[(m(X)-g(X))^2]$. Since the second term is the expectation of a non-negative variable, \n",
    "$$E[(Y-g(X))^2] \\geq E[(Y-m(X))^2]$$ for any function $g(X)$. This shows that $g(X)=m(X)$ is where the MSE is minimized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352fcca-1594-4468-9c97-8e355520e20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4d46f-cf0a-4b22-bbbb-4f9c22cbc2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
