\documentclass[a4paper, 11pt]{article}
\usepackage{amsmath}
\newcommand{\marginleft}{2.5cm}             % Left margin
\newcommand{\marginright}{2.5cm}            % Right margin
\newcommand{\margintop}{2.5cm}              % Top margin
\newcommand{\marginbottom}{2.5cm}             % Bottom margin
\usepackage[a4paper, bindingoffset=0.0cm, left=\marginleft, right=\marginright, top=\margintop, bottom=\marginbottom, footskip=.25in]{geometry}
\setlength{\parindent}{0pt}

\begin{document}

\textbf{1. Prove the Frisch–Waugh–Lovell theorem} \\

Given the model:
\begin{equation}
    y = D \beta_1 + W \beta_2 + \mu \label{eq1}
\end{equation}

where \(y\) is an \(n \times 1\) vector, \(D\) is an \(n \times k_1\) matrix, \(\beta_1\) is a \(k_1 \times 1\) vector, \(W\) is an \(n \times k_2\) matrix, \(\beta_2\) is a \(k_2 \times 1\) vector, and \(\mu\) is an \(n \times 1\) vector of error terms. \\

We can construct the following equation:
\begin{equation}
    \epsilon_y = \epsilon_D \phi + \xi
\end{equation}
where \(\epsilon_y\) is the vector of residuals from regressing \(y\) on \(W\), \(\epsilon_D\) is the matrix of residuals from regressing each column of \(D\) on \(W\), and \(e\) is an \(n \times 1\) vector of error terms. \\

Running \(y\) on \(W\) we get:
\begin{align}
    y &= W\hat{\alpha}_1 + \epsilon_y \iff \epsilon_y = y - W\hat{\alpha}_1
\end{align}

Similarly, running \(D\) on \(W\) gives us:
\begin{align}
    D &= W\hat{\alpha}_2 + \epsilon_D \iff \epsilon_D = D - W\hat{\alpha}_2
\end{align}

Running \(\epsilon_y\) on \(\epsilon_D\):
\begin{align}
    y - W \hat{\alpha}_1 &= (D - W \hat{\alpha}_2) \phi + \xi \nonumber \\
    y &= W \hat{\alpha}_1 + (D - W \hat{\alpha}_2) \phi + \xi \nonumber \\
    y &= W \hat{\alpha}_1 + D \phi - W \hat{\alpha}_2 \phi + \xi \nonumber \\
    y &= D \phi + W (\hat{\alpha}_1 - \hat{\alpha}_2 \phi) + \xi \label{eq5}
\end{align}

Comparing \eqref{eq1} with \eqref{eq5} we can see that:
\begin{align}
    \beta_1 &= \phi \\
    \beta_2 &= \hat{\alpha}_1 - \hat{\alpha}_2 \phi \\
    \mu &= \xi
\end{align}


\pagebreak
\textbf{2. Show that the Conditional Expectation Function minimizes the expected squared error.} \\

Given the model:
\begin{equation}
    Y = m(X) + e \label{eq9}
\end{equation}
Where \(m(X)\) represents the conditional expectation of \(Y\) on \(X\). Lets define the arbitrary model:
\begin{equation}
    Y = g(X) + w \label{eq10}
\end{equation}
Where \(g(X)\) represents any function of \(X\). \\

Working with the expected squared error from model \eqref{eq10}:

\begin{align}
    E[(Y-g(X))^2] =& E[(Y-m(X)+m(X)-g(X))^2] \nonumber \\
    =& E[(Y-m(X))^2+2(Y-m(X))(m(X)-g(X)) +(m(X)-g(X))^2] \nonumber \\
    =& E[(Y-m(X))^2] + 2E[(Y-m(X))(m(X)-g(X))] + E[(m(X)-g(X))^2] \nonumber \\
    =& E[(e^2)] + 2E[(Y-m(X))(m(X)-g(X))] + E[(m(X)-g(X))^2] \nonumber
\end{align}

Using the law of iterated expectations:

\begin{equation}
    E[(Y-g(X))^2] = E[(e^2)] + 2E[E[(Y-m(X))(m(X)-g(X))|X]] + E[(m(X)-g(X))^2] \nonumber 
\end{equation}

Since \(m(X)\) and \(g(X)\) are functions of \(X\), the term \((m(X)-g(X))\) can be thought as constant when conditioning on \(X\). Thus, we get:

\begin{align}
    E[(Y-g(X))^2] &= E[(e^2)] + 2E[E[Y-m(X)|X](m(X)-g(X))] + E[(m(X)-g(X))^2] \nonumber
\end{align}

It is important to note that \(E[Y-m(X)|X] = 0 \) by definition of \(m(X)\), so we get:

\begin{align}
    E[(Y-g(X))^2] &= E[(e^2)] + E[(m(X)-g(X))^2] \nonumber
\end{align}

Because the second term in the equation is always non-negative, it is clear that the function is minimized when \(g(X)\) equals \(m(X)\). In which case:

\begin{align}
    E[(Y-g(X))^2] &= E[(e^2)]
\end{align}
\end{document}