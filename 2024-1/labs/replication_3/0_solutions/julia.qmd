
## Ortogonal Learning

```{julia}
# !pip install multiprocess
# !git clone https://github.com/maxhuppertz/hdmpy.git
```

```{julia}
using Distributions, LinearAlgebra, DataFrames, GLM
using HDMjl


function gen_data(n::Int, p::Int)
    beta = (1 ./ (1:p).^2)
    gamma = (1 ./ (1:p).^2)

    mean = 0
    sd = 1
    X = rand(Normal(mean, sd), n, p)

    D = (X * gamma) + rand(Normal(mean, sd/4), n)
    
    Y = 10 .* D + (X * beta) + rand(Normal(mean, sd), n)

    return X, D, Y, beta, gamma
end

# function 

function naive_est(X, D, Y)
    XD = hcat(D, X)
    model = rlasso(XD, Y, post = true)
    coefs = model["coefficients"][2:end]
    SX_IDS = findall(coefs .!= 0)
    if isempty(SX_IDs)
        df = DataFrame(Y=Y, D=D)
        model = lm(@formula(Y ~ D), df)
        Naive = coef(model)[2]
    else
        X_D = hcat(D, X[:, SX_IDs])
        df = DataFrame(Y=Y, X_D)
        model = lm(@formula(Y ~ 0 + X_D), df)
        Naive = coef(model)[1]
    end
    return Naive
end

function ortho_est(X, D, Y)
    resY = rlasso(X, Y, post = false)["residuals"]
    resD = rlasso(X, D, post = false)["residuals"]
    df = DataFrame(y  = resY, d = resD)
    model = lm(@formula(y ~ d), df)
    ref = coef(model)[2]
    return ref
end

function simulate(X, D, Y)
    naive = naive_est(X, D, Y)
    orth = ortho_est(X, D, Y)
    return naive orth
end

function simulate_and_plot(B1)
    Random.seed!(0)
    Naive = zeros(B1)
    Orthogonal = zeros(B1)

    for i in 1:B1
        Naive[i], Orthogonal[i] = simulate()  
    end

    Orto_breaks = [-1.2, -1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2]
    Naive_breaks = [-0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2]

    p1 = histogram(Orthogonal .- 10, bins=Orto_breaks, normalize=true, xlims=(-2, 2), color=:blue, title="Orthogonal, $B1 trials", xlabel="Orthogonal")
    p2 = histogram(Naive .- 10, bins=Naive_breaks, normalize=true, xlims=(-2, 2), color=:red, title="Naive $B1 trials", xlabel="Naive")

    plot(p1, p2, layout=(1, 2), size=(800, 400))
end

```


```{julia}
simulate_and_plot(10)
simulate_and_plot(100)
simulate_and_plot(1000)
simulate_and_plot(10000)
```


```{julia}
using Dates  

s = now()
for i in 1:2
    simulate()
end
elapsed_time = now() - s
elapsed_time
```

```{julia}
using Base.Threads

function simulate_threaded(n)
    results = Vector{Any}(undef, n)
    @threads for i in 1:n
        results[i] = simulate()
    end
    return results
end

s = now()
simulate_threaded(2)
elapsed_time = now() - s
elapsed_time
```


## Doble Lasso

```{julia}
using CSV

url = "https://raw.githubusercontent.com/gsbDBI/ExperimentData/97a11199ba559f01c7b3803a1493ffa08631732e/School/bruhn2016.csv"
data = CSV.File(download(url)) |> DataFrame


dropmissing!(data)

rename!(data, Symbol.(replace.(string.(names(data)), "." => "_")))

names(data)
```

```{julia}
using Random


out_come = "outcome_test_score"
_d = "treatment"
cols = names(data)
xall = filter(col -> !(col in [Symbol(out_come), Symbol(_d)]), cols)


Random.seed!(12)

i_xall = randperm(length(xall))[1:2]
x_selected = xall[i_xall]


x_selected = [
 "has_computer_with_internet_at_home"
 "negotiates_prices_or_payment_methods"
 "failed_at_least_one_school_year"
]

```

```{julia}
X = select(data, [x_selected; _d])
y = data[!, Symbol(out_come)]
d = data[!, Symbol(_d_)]
```

### OLS

```{julia}

function ols_table_model(model, method)
    coefs = coef(model)
    ci = confint(model)
    result = DataFrame(
        l = ci[:, 1],
        u = ci[:, 2],
        coef = coefs,
        method = method
    )
    return result
end

function ols_table(data, m="ols", fmla = @formula(y ~ 0 + treatment))
    model = lm(fmla, data)
    return ols_table_model(model, method=m)
end

df = copy(X)
df.y = data[!, Symbol(out_come)]

ols = lm(@formula(y ~ 
        # treatment + 
        has_computer_with_internet_at_home + 
        negotiates_prices_or_payment_methods + failed_at_least_one_school_year + 
        treatment )
, df)
ols_result = ols_table_model(ols)
```


### Doble Lasso

```{julia}
y_fml = @formula(y ~ 
        # treatment + 
        has_computer_with_internet_at_home + 
        negotiates_prices_or_payment_methods + failed_at_least_one_school_year)
d_fml = @formula(treatment ~ 
        # treatment + 
        has_computer_with_internet_at_home + 
        negotiates_prices_or_payment_methods + failed_at_least_one_school_year)

col_real = "y"

function residual_lass(fml, data, col_real = "y")

    lasso_modl_ = fit(LassoModel, fml, data)
    real_y = data[!, Symbol(col_real)]
    res = real_y - predict(lasso_modl_)
    return res
end

lasso_cv_r_y = residual_lass(y_fml, df, "y")
lasso_cv_r_D = residual_lass(d_fml, df, "treatment")

df1 = DataFrame(y = lasso_cv_r_y, d = lasso_cv_r_D)
d_lasso = ols_table(df1, "Doble Lasso", @formula(y ~ 0 + d))

```


### Doble Lasso - theorical lambda

```{julia}
def t_residual_lasso(X, y, theorical_lambda = .5):
    model = rlasso(X, y, lambda_start = theorical_lamda)
    return model['residuals']

t_lass_r_y = t_residual_lasso(X1, y)
t_lass_r_d = t_residual_lasso(X1, d)
df2 = DataFrame(y = t_lass_r_y, d = t_lass_r_d)
t_lass_result = ols_table(df2, "Doble Lasso - Theorical lambda", formula(y ~ 0 + d))
t_lass_result
```

### Doble Lasso - method partilling out


```{julia}
d_lasso = rlassoEffect(x=X1, y=y, d=d, method="partialling out")
coef = d_lasso["alpha"]
se = d_lasso["se"]
_l = coef - 1.96 * se
_u = coef + 1.96 * se
d_lass_m = DataFrame(
    coef= coef, l= _l, u= _u, method= "Lasso - Partilling out",
    coef_name = "treatment"
)
```

### Coef Plot

```{julia}
df_concatenated = vcat(ols, d_lasso_result, t_lass_result, d_lass_m)
df_filtered = filter(row -> row.coef_name == "treatment", df_concatenated)

```

```{julia}
plt = plot(
    xlabel = "",
    ylabel = "Coef",
    xticks = (1:length(df_filtered.method), df_filtered.method),
    xrotation = 45,
    xtickfont = font(8, "Courier"),  # Adjust font size and family as needed
    size = (900, 600),
    legend = false,
)

# Plotting the point plot (similar to sns.pointplot in Python)
scatter!(
    1:length(df_filtered.method),  # x coordinates
    df_filtered.coef,  # y coordinates
    color = :black,
    markershape = :circle,
    markercolor = :black,
    markerstrokecolor = :black,
    label = "",
)

# Plotting error bars
errorbar!(
    1:length(df_filtered.method),  # x coordinates
    df_filtered.coef,  # y coordinates (center of error bars)
    yerrors = [df_filtered.coef .- df_filtered.l, df_filtered.u .- df_filtered.coef],  # y error sizes
    linecolor = :gray,
    bar_width = 0.3,
    capsize = 5,
    label = "",
)

# Customize plot appearance
title!("Coefficient Plot")
xlims!(0.5, length(df_filtered.method) + 0.5)  # Adjust x-axis limits
ylim(minimum(df_filtered.l) - 0.1, maximum(df_filtered.u) + 0.1)  # Adjust y-axis limits

# Display plot
display(plt)
```