## What is the research question of both articles?
The research question in both papers is the following: How to decide to include variables as control in a regression? Cinelli et al. (2022) focus on directed acyclic graphs (DAGs) and Hunermund et al. (2022) focus on how double machine learning (DML) is sensitive due to bad controls.
## What are the strengths and weaknesses of both papers' approaches to responding to that question?
The first paper shows graphically the criteria back-door, which is helpful to distinguish between good and bad controls mainly. In the case of neutral controls, although there is no effect on bias reduction, including them can improve or worsen the precision of the ACE estimation.  Because of this, as a rule, controls that are parents of explanatory variables (X) which are not required for identification are detrimental for the asymptotic variance of the estimator. Instead, controls that are not parents of X, but are of Y are beneficial as they improve precision of ACE estimation as well as reduce the variation of the dependent variable (outcome Y). On the other hand, for itself, in some cases, DAG structure cannot identify the causal effect of X on Y, so it does not determine if the variable Z should be included.

The second paper simulates four structural causal models and finds out that DML performs worse than naive Lasso for the m-graph, since bias DML is greater than bias Lasso. Additionally, the DML usually performs well when it works with good control case, even better that using naive Lasso.

## How do the papers advance knowledge about the question, i.e., what is the contribution? 
The first paper refers to provide a visual summary of the criteria to differentiate good controls from bad ones. About neutral controls, graphical tools help to understand the role that these variables play in a regression and their impact. For example, neutral control as post-treatment variable can be useful when selection bias exists. Additionally, “bad controls” (Z) in the scenery when X has no causal effect on Y can be include for verifying if the effect of X on Y is zero. Another contribution is that it offers graphical models to explain some research as “the birth-weight paradox” and “The Antebellum Puzzle”. 

The second paper shows the consequences of confounding variables and their effects for DML due to the inclusion of bad controls in the conditioning set. Besides, it analyzes both methods double machine learning and naive lasso to study how the inclusion of bad control affects the causal effect estimates (bias)

## What would be one or two specific and valuable next steps to move forward on this question?

Next steps about this topic is continue testing both lasso and DML in high-dimensional settings to apply in empirical cases.