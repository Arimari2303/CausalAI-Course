# Double/debiased machine learning for treatment and structural parameters

Ana Cristina Angulo Colina 20171627

- ## What is the research question of the article?

    In a context where we want to make inference on a low dimensional parameter $\theta$ in the presence of high dimensional nuisance parameters. The authors aim to answer the question: how can we remove the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta_0$? And to answer this question they the introduce two key “ingredients”: Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta_0$  and cross-fitting as a form for data-splitting.

- ## What are the strengths and weaknesses of the paper's approach to answering that question?

    One of the strengths of this paper is that using orthogonalization and data splitting, the authors are able to overcome the regularization bias. Moreover it is important to mention that using sample splitting one can obtain good results under very weak conditions and together sample splitting and orthogonalization allow us to establish good behavior of an estimator for $\theta_0$.
    Additionally, the paper not only presents the theoretical applications, but it also illustrates the use of DML in three empirical examples: the Pennsylvania Reemployment Bonus experiment (we are familiarized with this one), the effect of the 401(k) eligibility and the IV estimation of the effects of institutions on economics growth.
    However, one limitation I find is that in all the empirical examples shown in the paper the results where similar regardless of the method employed. 

- ## How does this document advance knowledge about the question, that is, what is the contribution?

    The contribution of this paper is that it presents a simple procedure for estimating and to perform inference on $\theta_0$ that is formally valid in these highly complex settings.  