# Report 9
### Angela Coapaza (20171636)

## Double/debiased machine learning for treatment and structural parameters


In this article, Chernozhukov et al (2018) aim to answer the semi-parametric problem of inference on a low-dimensional
parameter in the presence of high-dimensional nuisance parameters. To do this, the authors use double/debiased machine learning methods (MLD), which allow eliminating the influence of the regulatization bias and overfitting on estimation of the parameter of interest through the use of orthogonal Neyman moments and cross-fitting.

One of the strenghts (and contributions) of this article is that the authors succed in showing thatthe use of th double ML ortoghonal estimator is approximately unbiased  as opposed to the conventional (nonorthogonal) ML estimator, which is highly biased in relation to the real value of . In addition, they prove that the use of a cross-fitting procedure, which splits the sample, reduces the substantial bias from the full sample estimator and generates a histogram spread similar to that of the full sample estimator. 
Finally, three empirical examples are given (the Pennsylvania Reemployment Bond experiment, the effect of 401(k) eligibility, and the IV estimate of the effects of institutions on economic growth), which allow us to know how to apply the theory. proposed in the first sections of the article on empirical studies using real databases. Regarding the limitations, it was found that the mathematical notation can generate difficulties when reviewing the article, so this would be a point to improve.

The contribution of the article is that we can find that these examples conclude that all the methods used for estimating nuisance functions (including Lasso, Regression Trees, Boosting, Neural Network, Ensemble and Best) generate similar robustness results  and high-quality approximations to the underlying nuisance functions. This implies that the method proposed by the authors is robust.
Furthermore, we find that the uncertainty generated by the sample split does not significantly increase the standard errors compared to a baseline without this uncertainty, so the sample split is valid.

The next step should be to compare other types of machine learning methods to find out if there is another type of methodology that generates even more robust results.