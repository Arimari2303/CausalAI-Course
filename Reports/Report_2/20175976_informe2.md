Paper 2: Inference for High-Dimensional Sparse Econometric Models (Belloni, Chenozhukov & Hansen, 2011)
Student: Kevin Jaime Martinez
Code: 20175976
This paper by Belloni, Chernozhukov and Hansen investigates new estimation and inference methods in contexts where
there is a large presence of variables, and even in some cases where there are more variables than observed samples. 
Traditional econometric estimation models can have problems in the presence ofhigh-dimensional sparseness. 
An example of this is overfitting, which occurs when the training data is overfitted, which means that noise can be 
captured instead of real patterns of behavior. There can also be errors in the calculations of the parameters;
errors occur because the model has many parameters to estimate, but few data to rely on. Its calculations can be
imprecise and highly sensitive to variations in the data. In this sense, the paper advances research for high sparse 
dimensionality models, seeking to answer the question of how to improve the estimation and inference of those econometric
models and to do so it presents practical applications, in addition to an exhaustive explanation, these being its main strengths.

The authors analyze and explain the penalty models, which help to penalize, reducing the value of the parameters of 
irrelevant variables. 

For example, we have the Lasso ℓ1 penalty method (Least Absolute Shrinkage and Selection Operator). This helps to select the relevant 
variables, among the many that exist, through a penalty term ℓ1 to the parameters, which tends to directly eliminate estimated parameters 
that are below a limit.

The second is Ridge Regression, which penalizes the sum of squares of the coefficients ℓ2. It is used in the context 
of multicollinearity (high presence of correlation between variables) and high sparse dimensionality. This, however, does not 
force the parameters to zero like Lasso.
The other is Elastic Net, which combines the Lasso penalty and the Ridge Regression penalty. With Lasso, it helps to eliminate variables
that are not useful,
and with ℓ2, it selects a group of variables, which provides greater stability in models with correlated variables.
This paper has other strengths, such as being exhaustive in the sense of corroboration through simulations. For example,
for Lasso, simulations were carried out that eliminated variables that turned out to be noise, while those that were relevant were selected.
Another strength is that it investigates and offers practical applications of these three models or of regulation in high-dimensional 
sparse contexts.
According to the authors, it can be important for economic or financial prediction problems (macroeconomic variables, surveys, and financial
indicators that use a set of many variables).
Likewise, there are other applications such as the impact of policies (subsidies, regulations, etc.) that use a wide set of variables.
On the weak side, we have the computational cost, in the sense that the complexity involved in implementing these methods is not explored. 
On the other hand, we also have to mention that it does not deal with the cost that it represents for ordinary professionals to access
these methods, since their use requires extensive statistical knowledge.
Another weakness would be that it does not go beyond the 
application and corroboration of the usefulness of these methods. There are parts that are still missing to explain, such as the
interpretability of the data. Many economic, financial, etc. problems may require an enormous amount of variables that are not noise 
and that could be eliminated if an aggressive ℓ1 variable selection is carried out. The variable selection can be more complete 
and the misunderstanding of these can lead to inaccuracy.
As for the contributions, apart from those mentioned, such as the robustness they offer in a context of high sparse dimension 
for prediction and inference, the paper recognizes thatthere are occasions in which the Lasso can bias the prediction, due to the ℓ1 penalty. 
For this, it proposes the Post Lasso method or model. This is a complementary technique to the subsequent Lasso. It seeks not only 
to select variables, but also to obtain more precise coefficients, improving the robustness and interpretability 
of the final model; however, it also has weaknesses such as computational costs and its dependence on the first 
stage of Lasso.If the first stage is done poorly, the other will not be able  to significantly improve the precision of the parameters.
A further step that could be taken is to investigate some variants of Lasso, such as Group Lasso, which, instead of using individual variables,
selects grouped variables, each of them related to the same variable.
